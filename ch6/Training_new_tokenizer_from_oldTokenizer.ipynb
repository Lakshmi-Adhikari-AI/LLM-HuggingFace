{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLTrgEwp8NqV"
      },
      "source": [
        "# üèóÔ∏è  Training a new tokenizer from an old one\n",
        "Sometimes you need to train a tokenizer for a new domain or language. Tokenizer training is a deterministic, statistical process‚Äîunlike neural model training.  \n",
        "Here, we‚Äôll train a GPT-2-style tokenizer for Python code using CodeSearchNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXKrBMOv8NqY"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRQ_xaCt8NqY"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjKiXDGg8NqZ"
      },
      "source": [
        "You will need to setup git, adapt your email and name in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Um-x1qDc8Nqa"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"lakshmi.adhikari26@gmail.com\"\n",
        "!git config --global user.name \"Lakshmi-Adhikari-AI\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbDWGyll8Nqa"
      },
      "source": [
        "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za7FUBRr8Nqa"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show datasets\n"
      ],
      "metadata": {
        "id": "TT4QaeJb_H2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets==3.6.0\n"
      ],
      "metadata": {
        "id": "zDGb2yBP_NUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ Assemble a Training Corpus using ü§ó Datasets\n",
        "\n",
        "Let's load and inspect the Python part of CodeSearchNet‚Äîa large dataset of Python functions from GitHub.\n"
      ],
      "metadata": {
        "id": "-exYY6bN9dBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load CodeSearchNet Python corpus; may take a few minutes\n",
        "raw_datasets = load_dataset(\"code_search_net\", \"python\")\n"
      ],
      "metadata": {
        "id": "3Z__Qb_d9lYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GXsDJFE8Nqa"
      },
      "outputs": [],
      "source": [
        "# Inspect the training split's columns and size\n",
        "print(raw_datasets[\"train\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Preview Sample Function Strings\n",
        "\n",
        "We'll use the \"whole_func_string\" column (entire function as text) to train our tokenizer.\n"
      ],
      "metadata": {
        "id": "aoPwCZiA_xxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print an example Python function for context\n",
        "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
      ],
      "metadata": {
        "id": "RGzBpMsb_61G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Efficiently Prepare an Iterable Corpus\n",
        "\n",
        "Break up the dataset into batches (e.g., 1,000 functions at a time), using a generator to avoid loading everything into RAM.\n"
      ],
      "metadata": {
        "id": "5OhB6PaTATrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_corpus():\n",
        "  dataset=raw_datasets[\"train\"]\n",
        "  for start_idx in range(0,len(dataset),1000):\n",
        "    samples=dataset[start_idx:start_idx+1000]\n",
        "    yield samples[\"whole_func_string\"]\n",
        "\n",
        "# This object only loads data as needed, perfect for huge datasets\n",
        "training_corpus=get_training_corpus()"
      ],
      "metadata": {
        "id": "EpCQrCTJAjbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Load the Existing (GPT-2) Tokenizer\n",
        "\n",
        "Start from a pretrained tokenizer so we keep existing behavior and special tokens.\n"
      ],
      "metadata": {
        "id": "eD6r22xtBUyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load 'GPT-2's tokenizer as a starting point\n",
        "old_tokenizer=AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "nQ2HBHlmBYv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Try the Old Tokenizer on Code\n",
        "\n",
        "How does the original vocabulary handle our Python domain? It's not very efficient!\n"
      ],
      "metadata": {
        "id": "c6EONnWPBu3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example='''def add_numbers(a,b):\n",
        "  \"\"\"Add the two numbers 'a' and 'b'.\"\"\"\n",
        "  return a+b'''\n",
        "\n",
        "# See how \"GPT-2 English splits up a python  function\"\n",
        "tokens=old_tokenizer.tokenize(example)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "O2ACvgzpBw_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6Ô∏è‚É£ Train a New Tokenizer, Adapted to Code\n",
        "\n",
        "We'll use train_new_from_iterator() to retrain the vocabulary for our specific corpus.\n"
      ],
      "metadata": {
        "id": "OAnBjDjQCuIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train  a tokenizer with a vocab size of 52,000 (recommended for large corpora)\n",
        "tokenizer=old_tokenizer.train_new_from_iterator(training_corpus,52000)"
      ],
      "metadata": {
        "id": "NR1ZgghBC1RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7Ô∏è‚É£ Try the New Tokenizer on the Same Code\n",
        "\n",
        "Does it do better with indentation, underscores, and other Python syntax? Let's check!\n"
      ],
      "metadata": {
        "id": "evwj5dG6DNwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens=tokenizer.tokenize(example)\n",
        "print(tokens) # More compact,domain-awate tokens!\n",
        "print(len(tokens)) # Should be fewer than before (more efficient subwords)\n",
        "print(len(old_tokenizer.tokenize(example)))"
      ],
      "metadata": {
        "id": "1n-y7kBGDWoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8Ô∏è‚É£ Test on Another Code Example\n",
        "\n",
        "Check its handling of indents, underscores, camel case, and more.\n"
      ],
      "metadata": {
        "id": "MUdBMKY0DyU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example2=\"\"\"class LinearLayer():\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weight = torch.randn(input_size, output_size)\n",
        "        self.bias = torch.zeros(output_size)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return x @ self.weights + self.bias\n",
        "      \"\"\"\n",
        "print(tokenizer.tokenize(example2))"
      ],
      "metadata": {
        "id": "5y0224z0D0aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9Ô∏è‚É£ Save and Share the Tokenizer\n",
        "\n",
        "Preserve your trained tokenizer for future work, sharing, or fine-tuning.\n"
      ],
      "metadata": {
        "id": "blEVx2frECo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
      ],
      "metadata": {
        "id": "zrwPrutrEGzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîü Push the Tokenizer to the Hugging Face Hub\n",
        "\n",
        "Upload with authentication so anyone can reuse it.\n"
      ],
      "metadata": {
        "id": "cCCjC-kEEQB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "id": "6FqAkLgNETyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.push_to_hub(\"code-search-net-tokenizer\")"
      ],
      "metadata": {
        "id": "dPvFiBqBEi_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£1Ô∏è‚É£ Load your Tokenizer Anywhere\n",
        "\n",
        "Anyone can now use:\n"
      ],
      "metadata": {
        "id": "g-Rqv6n8Eusd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=AutoTokenizer.from_pretrained(\"your-username/code-search-net-tokenizer\")"
      ],
      "metadata": {
        "id": "ScuxzVzUExiV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Training a new tokenizer from an old one",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}