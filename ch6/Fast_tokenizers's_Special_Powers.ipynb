{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lakshmi-Adhikari-AI/LLM-HuggingFace/blob/main/ch6/Fast_tokenizers's_Special_Powers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goZPKiHz7lNn"
      },
      "source": [
        "#⚡ Fast tokenizers' special powers (PyTorch)\n",
        "The Hugging Face `AutoTokenizer` fast variant (backed by Rust) unlocks high-speed processing and powerful text-to-token, token-to-text mappings.  \n",
        "Let's explore how to inspect tokens, map them to words/chars, and replicate the outputs of token-classification/NER pipelines.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it5qGdS-7lNp"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLHu4qCN7lNp"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1️⃣ Fast vs Slow Tokenizers: Why the Difference Matters\n",
        "\n",
        "Fast tokenizers accelerate batch tokenization, and keep track of which original text spans produce which tokens (offset mapping).\n"
      ],
      "metadata": {
        "id": "-wVIkueo8PSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "example=\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
        "encoding=tokenizer(example)\n",
        "\n",
        "print(type(encoding)) # Output: BatchEncoding, a dictionary subclass\n",
        "print(tokenizer.is_fast) # True: we're using a fast tokenizer\n",
        "print(encoding.is_fast) # Also True!\n",
        "print(encoding)"
      ],
      "metadata": {
        "id": "uo2sUlBc8Enk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2️⃣ Exploring Token-Level Information\n",
        "\n",
        "Fast tokenizers return lots of useful information. Let's see the token IDs for a sentence, the original word indices, and how tokens map back to original text.\n"
      ],
      "metadata": {
        "id": "kIKmJEUo9rIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List tokens (subword units) produced by the tokenizer\n",
        "print(encoding.tokens())\n",
        "\n",
        "# Word IDs: which word in the original sentence does each token come from\n",
        "print(encoding.word_ids())\n",
        "\n",
        "# Example: For word index 3 (i.e,\"Sylvain\") what is the text span?\n",
        "start,end=encoding.word_to_chars(3)\n",
        "print(start)\n",
        "print(end)\n",
        "print(example[start:end]) # Should print \"Sylvain\""
      ],
      "metadata": {
        "id": "F1sgNWmw9xYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3️⃣ Try Comparing Tokenizers on a Tricky Input\n",
        "\n",
        "What happens if you tokenize \"81s\" using BERT and RoBERTa?\n"
      ],
      "metadata": {
        "id": "J0cn1934-9eZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_bert=AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "tokenizer_roberta=AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "text=\"81s\"\n",
        "print(tokenizer_bert.tokenize(text)) # BERT splits into ['81','s']\n",
        "print(tokenizer_bert(example,return_tensors=\"pt\").tokens())\n",
        "print(tokenizer_roberta.tokenize(text)) # RoBERTa may treat differently\n",
        "print(tokenizer_roberta(example,return_tensors=\"pt\").tokens())\n"
      ],
      "metadata": {
        "id": "I_PUyrpo_FWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4️⃣ Inside the Token-Classification/Ner Pipeline\n",
        "\n",
        "Let’s run NER with Hugging Face pipeline to see how token/word/entity predictions are made, then try to replicate it manually.\n"
      ],
      "metadata": {
        "id": "oSB7yg-KAfmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "token_classifier=pipeline(\"token-classification\")\n",
        "print(token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"))\n",
        "\n",
        "token_classifier=pipeline(\"token-classification\",aggregation_strategy=\"simple\")\n",
        "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ],
      "metadata": {
        "id": "Srbe9socAnad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5️⃣ Get Predictions Manually: Model, Tokenizer, Input\n",
        "\n",
        "Let's tokenize, run the model, and decode entity predictions ourselves.\n"
      ],
      "metadata": {
        "id": "uN8HBmPcCT_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model_checkpoint=\"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model=AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
        "\n",
        "inputs=tokenizer(example,return_tensors=\"pt\")\n",
        "print(inputs)\n",
        "outputs=model(**inputs)\n",
        "print(inputs[\"input_ids\"].shape) # batch_size=1,seq_len=19\n",
        "print(outputs.logits.shape) # (batch_size=1,seq_len=19,num_labels=9)"
      ],
      "metadata": {
        "id": "_cWRKXm2CWkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6️⃣ From Model Outputs to Entity Predictions\n",
        "\n",
        "Convert model logits to softmax probabilities, then use argmax to pick the highest-prob label for each token.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v4ugBHfUDpF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "probabilities=torch.nn.functional.softmax(outputs.logits,dim=-1)[0].tolist()\n",
        "predictions=outputs.logits.argmax(dim=-1)[0].tolist()\n",
        "\n",
        "print(predictions) # Each index into model.config.id2label\n",
        "print(model.config.id2label)"
      ],
      "metadata": {
        "id": "sCstKJsVDyyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7️⃣ Display Non-Outsider Tokens\n",
        "\n",
        "Extract tokens where the predicted label is not \"O\" (outside any entity).\n"
      ],
      "metadata": {
        "id": "4VrJzCsKEnPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "tokens = inputs.tokens()\n",
        "\n",
        "for idx, pred in enumerate(predictions):\n",
        "    label = model.config.id2label[pred]\n",
        "    if label != \"O\":\n",
        "        results.append(\n",
        "            {\"entity\": label, \"score\": probabilities[idx][pred], \"word\": tokens[idx]}\n",
        "        )\n",
        "\n",
        "print(results)"
      ],
      "metadata": {
        "id": "N1TaKoYwEuaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8️⃣ Add Character Spans With Offsets\n",
        "\n",
        "Include the character boundaries from the original text for each entity token.\n"
      ],
      "metadata": {
        "id": "pFueFCc8F1pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_with_offsets=tokenizer(example,return_offsets_mapping=True)\n",
        "print(inputs_with_offsets[\"offset_mapping\"])\n",
        "print(example[12:14])\n",
        "\n",
        "tokens=inputs_with_offsets.tokens()\n",
        "offsets=inputs_with_offsets[\"offset_mapping\"]\n",
        "\n",
        "result=[]\n",
        "for idx,pred in enumerate(predictions):\n",
        "  label=model.config.id2label[pred]\n",
        "  if label!=\"O\":\n",
        "    start,end=offsets[idx]\n",
        "    results.append({\n",
        "        \"entity\":label,\n",
        "        \"score\":probabilities[idx][pred],\n",
        "        \"word\":tokens[idx],\n",
        "        \"start\":start,\n",
        "        \"end\":end,\n",
        "    })\n",
        "print(results)"
      ],
      "metadata": {
        "id": "Yzahnl8AF4hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9️⃣ Group Entities (e.g. Multi-token \"PER\", \"ORG\", \"LOC\")\n",
        "\n",
        "Offsets let us group tokens into full entities directly in the original text span—no need to join tokens manually!\n"
      ],
      "metadata": {
        "id": "05XPmpi5HtS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "results = []\n",
        "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
        "tokens = inputs_with_offsets.tokens()\n",
        "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
        "\n",
        "idx = 0\n",
        "while idx < len(predictions):\n",
        "    pred = predictions[idx]\n",
        "    label = model.config.id2label[pred]\n",
        "    if label != \"O\":\n",
        "        # Remove the B- or I-\n",
        "        label = label[2:]\n",
        "        start, _ = offsets[idx]\n",
        "\n",
        "        # Grab all the tokens labeled with I-label\n",
        "        all_scores = []\n",
        "        while (\n",
        "            idx < len(predictions)\n",
        "            and model.config.id2label[predictions[idx]] == f\"I-{label}\"\n",
        "        ):\n",
        "            all_scores.append(probabilities[idx][pred])\n",
        "            _, end = offsets[idx]\n",
        "            idx += 1\n",
        "\n",
        "        # The score is the mean of all the scores of the tokens in that grouped entity\n",
        "        score = np.mean(all_scores).item()\n",
        "        word = example[start:end]\n",
        "        results.append(\n",
        "            {\n",
        "                \"entity_group\": label,\n",
        "                \"score\": score,\n",
        "                \"word\": word,\n",
        "                \"start\": start,\n",
        "                \"end\": end,\n",
        "            }\n",
        "        )\n",
        "    idx += 1\n",
        "\n",
        "print(results)"
      ],
      "metadata": {
        "id": "I1X-OCUxH1jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🎯 Summary\n",
        "\n",
        "Fast tokenizers make entity extraction, labeling, and mapping between token IDs and text spans efficient and easy.  \n",
        "This is essential for token classification, NER, QA, and custom NLP tasks—unlocking batch speed and rich mapping features.\n"
      ],
      "metadata": {
        "id": "xUN1-2m6Jnw2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Fast tokenizers' special powers (PyTorch)",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}