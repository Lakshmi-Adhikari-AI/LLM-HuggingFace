{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDcSxB2Uk1Cv"
      },
      "source": [
        "# üè∑Ô∏è WordPiece tokenization\n",
        "\n",
        "WordPiece is the subword algorithm Google developed for BERT and family.  \n",
        "It's similar to BPE but chooses merges by a scoring function and encodes words by greedy longest-match."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qreDIfuk1Cw"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDFJgeM3k1Cx"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ WordPiece Training Algorithm: How It Works\n",
        "\n",
        "- Start with base vocabulary (special tokens, initial alphabet).\n",
        "- Insert prefix (e.g. `##`) to indicate continuation subwords.\n",
        "- Learn merges by scoring pairs:\n",
        "  score = freq(pair) / (freq(first) * freq(second))\n",
        "- Merge pairs with best scores until vocabulary size is reached.\n"
      ],
      "metadata": {
        "id": "QLfSa7oGlhkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Example Corpus and Pre-tokenization\n",
        "\n",
        "We reuse the corpus and build word frequencies just like the BPE example.\n"
      ],
      "metadata": {
        "id": "9cwOXpVkll1u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn1rmJJAk1Cx"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"This is the Hugging Face Course.\",\n",
        "    \"This chapter is about tokenization.\",\n",
        "    \"This section shows several tokenizer algorithms.\",\n",
        "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from collections import defaultdict\n",
        "\n",
        "# Pre-tokenize with a WordPiece-style (BERT) tokenizer\n",
        "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "word_freqs=defaultdict(int)\n",
        "for text in corpus:\n",
        "  words_with_offsets=tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "  new_words=[word for word,offset in words_with_offsets]\n",
        "  for word in new_words:\n",
        "    word_freqs[word]+=1\n",
        "\n",
        "print(word_freqs) # Shows words and counts\n",
        ""
      ],
      "metadata": {
        "id": "knq0l_dhlreX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Build the Initial Alphabet\n",
        "\n",
        "For WordPiece, use each word‚Äôs first character and then all other characters prefixed by '##'.\n"
      ],
      "metadata": {
        "id": "QT2iJR_8nlYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alphabet=[]\n",
        "for word in word_freqs.keys():\n",
        "  if word[0] not in alphabet:\n",
        "    alphabet.append(word[0])\n",
        "  for letter in word[1:]:\n",
        "    if f\"##{letter} not in alphabet\":\n",
        "      alphabet.append(f\"##{letter}\")\n",
        "\n",
        "alphabet.sort()\n",
        "print(alphabet) # e.g. ['##a', '##b', ..., 'C', 'F', ..., 'a', 'b', 'c']"
      ],
      "metadata": {
        "id": "SBbmafN7nox8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Start Vocabulary with Special Tokens and Initial Alphabet\n",
        "\n",
        "Include special tokens for BERT, then the alphabet.\n"
      ],
      "metadata": {
        "id": "NKyvRE9co1zM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()\n"
      ],
      "metadata": {
        "id": "CoeyBn1mo3hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Split Words for Training\n",
        "\n",
        "Each word gets split to first char (no prefix), other chars with '##'.\n"
      ],
      "metadata": {
        "id": "E1Gcf5NzpDoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splits={\n",
        "    word:[c if i==0 else f\"##{c}\" for i,c in enumerate(word)]\n",
        "    for word in word_freqs.keys()\n",
        "}\n",
        "splits"
      ],
      "metadata": {
        "id": "jkhBvuO4pIYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6Ô∏è‚É£ Compute Pair Scores\n",
        "\n",
        "Track letter frequencies and pair frequencies. Score each pair as freq(pair) / (freq(first) * freq(second)).\n"
      ],
      "metadata": {
        "id": "Hkblmx1dpgyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_pair_scores(splits):\n",
        "    letter_freqs = defaultdict(int)\n",
        "    pair_freqs = defaultdict(int)\n",
        "    for word, freq in word_freqs.items():\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            letter_freqs[split[0]] += freq\n",
        "            continue\n",
        "        for i in range(len(split) - 1):\n",
        "            pair = (split[i], split[i + 1])\n",
        "            letter_freqs[split[i]] += freq\n",
        "            pair_freqs[pair] += freq\n",
        "        letter_freqs[split[-1]] += freq\n",
        "\n",
        "    scores = {\n",
        "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
        "        for pair, freq in pair_freqs.items()\n",
        "    }\n",
        "    return scores\n",
        "\n",
        "pair_scores=compute_pair_scores(splits)\n",
        "for i,key in enumerate(list(pair_scores.keys())[:6]):\n",
        "  print(f\"{key}:{pair_scores[key]}\")\n",
        "# Example: ('T', '##h'): 0.125, ('##h', '##i'): 0.03, etc.\n"
      ],
      "metadata": {
        "id": "hxZ8-8gypoiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_pair = \"\"\n",
        "max_score = None\n",
        "for pair, score in pair_scores.items():\n",
        "    if max_score is None or max_score < score:\n",
        "        best_pair = pair\n",
        "        max_score = score\n",
        "\n",
        "print(best_pair, max_score)"
      ],
      "metadata": {
        "id": "uz5r29tuyA9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.append(\"ab\")"
      ],
      "metadata": {
        "id": "tGROCeXuyGxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7Ô∏è‚É£ Training Loop: Select and Merge Best Pairs (by Score)\n",
        "\n",
        "Loop for desired vocabulary size, pick best-scoring merge, and update all splits.\n"
      ],
      "metadata": {
        "id": "qXitvvDhuQEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_pair(a, b, splits):\n",
        "    for word in word_freqs:\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "        i = 0\n",
        "        while i < len(split) - 1:\n",
        "            if split[i] == a and split[i + 1] == b:\n",
        "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
        "                split = split[:i] + [merge] + split[i + 2 :]\n",
        "            else:\n",
        "                i += 1\n",
        "        splits[word] = split\n",
        "    return splits\n",
        "\n",
        "vocab_size = 70\n",
        "while len(vocab) < vocab_size:\n",
        "    scores = compute_pair_scores(splits)\n",
        "    best_pair, max_score = \"\", None\n",
        "    for pair, score in scores.items():\n",
        "        if max_score is None or max_score < score:\n",
        "            best_pair = pair\n",
        "            max_score = score\n",
        "    splits = merge_pair(*best_pair, splits)\n",
        "    new_token = (\n",
        "        best_pair[0] + best_pair[1][2:]\n",
        "        if best_pair[1].startswith(\"##\")\n",
        "        else best_pair[0] + best_pair[1]\n",
        "    )\n",
        "    vocab.append(new_token)\n",
        "print(vocab)  # All learned tokens\n"
      ],
      "metadata": {
        "id": "Rlv3uikluRaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8Ô∏è‚É£ WordPiece Tokenization Algorithm\n",
        "\n",
        "Given a new word, greedily select the longest matching subword from vocab, prefix next part with '##', repeat.\n"
      ],
      "metadata": {
        "id": "5m9OSIRyxAjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_word(word):\n",
        "    tokens = []\n",
        "    while len(word) > 0:\n",
        "        i = len(word)\n",
        "        while i > 0 and word[:i] not in vocab:\n",
        "            i -= 1\n",
        "        if i == 0:\n",
        "            return [\"[UNK]\"]\n",
        "        tokens.append(word[:i])\n",
        "        word = word[i:]\n",
        "        if len(word) > 0:\n",
        "            word = f\"##{word}\"\n",
        "    return tokens\n",
        "\n",
        "# Test with known and unknown words\n",
        "print(encode_word(\"Hugging\"))  # ['Hugg', '##i', '##n', '##g']\n",
        "print(encode_word(\"HOgging\"))  # ['[UNK]']\n",
        "\n"
      ],
      "metadata": {
        "id": "Cs8HR7vexCO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLRqSOFgk1C1"
      },
      "outputs": [],
      "source": [
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAsFWSZuk1C1"
      },
      "outputs": [],
      "source": [
        "def encode_word(word):\n",
        "    tokens = []\n",
        "    while len(word) > 0:\n",
        "        i = len(word)\n",
        "        while i > 0 and word[:i] not in vocab:\n",
        "            i -= 1\n",
        "        if i == 0:\n",
        "            return [\"[UNK]\"]\n",
        "        tokens.append(word[:i])\n",
        "        word = word[i:]\n",
        "        if len(word) > 0:\n",
        "            word = f\"##{word}\"\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBVy7JHlk1C1"
      },
      "outputs": [],
      "source": [
        "print(encode_word(\"Hugging\"))\n",
        "print(encode_word(\"HOgging\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Arh9OAuk1C1"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
        "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
        "    return sum(encoded_words, [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e-_8ORdk1C2"
      },
      "outputs": [],
      "source": [
        "tokenize(\"This is the Hugging Face course!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9Ô∏è‚É£ Full Text Tokenization\n",
        "\n",
        "Tokenize a sentence by pre-tokenizing and then applying the WordPiece strategy to each word.\n"
      ],
      "metadata": {
        "id": "zuVsMgwByf8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
        "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
        "    return sum(encoded_words, [])\n",
        "\n",
        "print(tokenize(\"This is the Hugging Face course!\"))\n",
        "# See BERT-style subword tokenization\n"
      ],
      "metadata": {
        "id": "Khw65-rFykAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ Summary\n",
        "\n",
        "- WordPiece builds its vocabulary by scoring possible merges, not just raw frequency.\n",
        "- Tokenization uses a greedy, longest-match-first search for known subwords, with '##' indicating word continuations.\n",
        "- Out-of-vocab words become '[UNK]', unlike BPE which can produce partially-known tokens.\n",
        "- This approach compactly covers English and many other languages, and is central to models like BERT.\n"
      ],
      "metadata": {
        "id": "SCO0FtFIytYk"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "WordPiece tokenization",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}