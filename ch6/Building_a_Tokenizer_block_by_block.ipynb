{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWwaH_72cmBs"
      },
      "source": [
        "# üõ†Ô∏è Building a tokenizer, block by block\n",
        "\n",
        "A tokenizer is built from modular components:\n",
        "- Normalization\n",
        "- Pre-tokenization\n",
        "- Model/trainer setup\n",
        "- Post-processing (special tokens, templates)\n",
        "- Decoding\n",
        "\n",
        "Let‚Äôs assemble WordPiece, BPE (byte-level), and Unigram tokenizers from scratch with the Tokenizers library!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuBHMOyocmBv"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHpeMz2_cmBw"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ Acquire Training Corpus (WikiText-2 for Demo Speed)\n",
        "\n",
        "Load and batch a public corpus as input for training tokenizers.\n"
      ],
      "metadata": {
        "id": "cwHXB_jadHQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset=load_dataset(\"wikitext\",name=\"wikitext-2-raw-v1\",split=\"train\")\n",
        "\n",
        "def get_training_corpus():\n",
        "  for i in range(0,len(dataset),1000):\n",
        "    yield dataset[i:i+1000][\"text\"]\n",
        "\n",
        "# Optionally,save texts to a file for direct training\n",
        "with open (\"wikitext-2.txt\",\"w\",encoding=\"utf-8\") as f:\n",
        "  for i in range(len(dataset)):\n",
        "    f.write(dataset[i][\"text\"]+\"\\n\")\n"
      ],
      "metadata": {
        "id": "Sqj1Xcf7dNAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Build a WordPiece (BERT-style) Tokenizer, Step by Step\n",
        "\n",
        "Let‚Äôs assemble all its main blocks‚Äîmodel, normalizer, pre-tokenizer, post-processor, decoder.\n"
      ],
      "metadata": {
        "id": "G28T1X-afrTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import(\n",
        "    decoders,models,normalizers,pre_tokenizers,processors,trainers,Tokenizer\n",
        ")\n",
        "\n",
        "# 2.1 Model: WordPiece with unk token\n",
        "tokenizer=Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
        "\n",
        "# 2.2 Normalization: Lowercaser + accents remover\n",
        "tokenizer.normalizer=normalizers.Sequence([\n",
        "    normalizers.NFD(), # Unicode normalization\n",
        "    normalizers.Lowercase(), # Lowercase\n",
        "    normalizers.StripAccents(), # Remove accents\n",
        "])\n",
        "\n",
        "print(tokenizer.normalizer.normalize_str(\"H√©ll√≤ h√¥w are √º?\"))  # Should print: hello how are u?\n",
        "\n",
        "# 2.3 Pre-tokenizer: split on whitespace and punctuation\n",
        "tokenizer.pre_tokenizer=pre_tokenizers.Sequence([\n",
        "    pre_tokenizers.WhitespaceSplit(),\n",
        "    pre_tokenizers.Punctuation()\n",
        "])\n",
        "print(tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\"))\n",
        "\n",
        "# 2.4 Trainer: WordPieceTrainer\n",
        "special_tokens=[\"[UNK]\",\"[PAD]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
        "trainer=trainers.WordPieceTrainer(vocab_size=25000,special_tokens=special_tokens)\n",
        "\n",
        "# 2.5 Train from iterator\n",
        "tokenizer.train_from_iterator(get_training_corpus(),trainer=trainer)"
      ],
      "metadata": {
        "id": "w2iozilLfvAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Post-Processing Template for Special Tokens\n",
        "\n",
        "Add [CLS]/[SEP] formatting for single and sentence-pair cases.\n"
      ],
      "metadata": {
        "id": "cwc7zwLRimjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cls_token_id=tokenizer.token_to_id(\"[CLS]\")\n",
        "sep_token_id=tokenizer.token_to_id(\"[SEP]\")\n",
        "print(cls_token_id,sep_token_id)\n",
        "\n",
        "tokenizer.post_processor=processors.TemplateProcessing(\n",
        "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
        "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
        "    special_tokens=[(\"[CLS]\",cls_token_id),(\"[SEP]\",sep_token_id)]\n",
        ")\n",
        "\n",
        "# 3.1 Test the template\n",
        "encoding=tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)\n",
        "\n",
        "encoding=tokenizer.encode(\"Let's test this tokenizer...\",\"on a pair of sentences.\")\n",
        "print(encoding.tokens)\n",
        "print(encoding.type_ids)"
      ],
      "metadata": {
        "id": "cJt5MIi_i06t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Add a Decoder\n",
        "\n",
        "Transform token IDs back to human-readable text.\n"
      ],
      "metadata": {
        "id": "CccuFr5Kk0tO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decoder=decoders.WordPiece(prefix=\"##\")\n",
        "decoded=tokenizer.decode(encoding.ids)\n",
        "print(decoded) # Should print: \"let's test this tokenizer... on a pair of sentences.\""
      ],
      "metadata": {
        "id": "fPUp2hXXk8q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Save & Reload\n",
        "\n",
        "Save to JSON, reload to verify portability.\n"
      ],
      "metadata": {
        "id": "LCcETY7mlRi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save(\"tokenizer.json\")\n",
        "from tokenizers import Tokenizer as LoadTokenizer\n",
        "new_tokenizer=LoadTokenizer.from_file(\"tokenizer.json\")\n"
      ],
      "metadata": {
        "id": "yUtpEjLPlUpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6Ô∏è‚É£ Make Transformer-Compatible: PreTrainedTokenizerFast\n",
        "\n",
        "Wrap your custom tokenizer for use in ü§ó Transformers API.\n"
      ],
      "metadata": {
        "id": "cT2bcYjAly3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "wrapped_tokenizer=PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    unk_token=\"[UNK]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    cls_token=\"[CLS]\",\n",
        "    sep_token=\"[SEP]\",\n",
        "    mask_token=\"[MASK]\",\n",
        ")"
      ],
      "metadata": {
        "id": "eIJTeMRKl5uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7Ô∏è‚É£ Build a ByteLevel BPE Tokenizer (GPT-2)\n",
        "\n",
        "Skip normalizer, use byte-level pre-tokenizer and trainer, and add byte-level post-processor and decoder.\n"
      ],
      "metadata": {
        "id": "mVdgZdFZmltU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer(models.BPE())\n",
        "tokenizer.pre_tokenizer=pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
        "trainer=trainers.BpeTrainer(vocab_size=25000,special_tokens=[\"<|endoftext|>\"])\n",
        "tokenizer.train_from_iterator(get_training_corpus(),trainer=trainer)\n",
        "tokenizer.post_processor=processors.ByteLevel(trim_offsets=False)\n",
        "tokenizer.decoder=decoders.ByteLevel()\n",
        "\n",
        "encoding=tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)\n",
        "print(tokenizer.decode(encoding.ids))"
      ],
      "metadata": {
        "id": "enwhhLGHmrCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8Ô∏è‚É£ Build a Unigram Tokenizer (XLNet)\n",
        "\n",
        "Use Metaspace pre-tokenizer, normalization, UnigramTrainer, and appropriate template for XLNet style.\n"
      ],
      "metadata": {
        "id": "-XCOQetXn3DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer(models.Unigram())\n",
        "from tokenizers import Regex\n",
        "\n",
        "tokenizer.normalizer=normalizers.Sequence([\n",
        "    normalizers.Replace(\"``\",'\"'),\n",
        "    normalizers.Replace(\"''\",'\"'),\n",
        "    normalizers.NFKD(),\n",
        "    normalizers.StripAccents(),\n",
        "    normalizers.Replace(Regex(\" {2,}\"), \" \")\n",
        "])\n",
        "tokenizer.pre_tokenizer=pre_tokenizers.Metaspace()\n",
        "special_tokens=[\"<cls>\",\"<sep>\",\"<unk>\",\"<mask>\",\"<s>\",\"</s>\"]\n",
        "trainer=trainers.UnigramTrainer(\n",
        "    vocab_size=25000,special_tokens=special_tokens,unk_token=\"<unk>\"\n",
        ")\n",
        "tokenizer.train_from_iterator(get_training_corpus(),trainer=trainer)\n",
        "encoding=tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)\n",
        "\n",
        "cls_token_id=tokenizer.token_to_id(\"<cls>\")\n",
        "sep_token_id=tokenizer.token_to_id(\"<sep>\")\n",
        "tokenizer.post_processor=processors.TemplateProcessing(\n",
        "    single=\"$A:0 <sep>:0 <cls>:2\",\n",
        "    pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\",\n",
        "    special_tokens=[(\"<sep>\", sep_token_id), (\"<cls>\", cls_token_id)],\n",
        ")\n",
        "encoding=tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences!\")\n",
        "print(encoding.tokens)\n",
        "print(encoding.type_ids)\n",
        "\n",
        "tokenizer.decoder=decoders.Metaspace()\n",
        "print(tokenizer.decode(encoding.ids))"
      ],
      "metadata": {
        "id": "ukdtEnlZoEmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ Summary\n",
        "\n",
        "- Build and mix tokenizer components: normalization, pre-tokenization, model/trainer, post-processing, decoding.\n",
        "- Try any model: WordPiece (BERT), ByteLevel BPE (GPT-2), Unigram (XLNet).\n",
        "- Save, reload, and wrap for use with Transformers."
      ],
      "metadata": {
        "id": "m41yTB_xrGVF"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Building a tokenizer, block by block",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}