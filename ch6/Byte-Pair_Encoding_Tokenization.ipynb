{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lakshmi-Adhikari-AI/LLM-HuggingFace/blob/main/ch6/Byte-Pair_Encoding_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCr4cTgWSac5"
      },
      "source": [
        "# üß© Byte-Pair Encoding tokenization\n",
        "BPE started as a text compression algorithm and later became the backbone of tokenizers in models like GPT, GPT-2, RoBERTa, BART, and DeBERTa.  \n",
        "This notebook walks you through BPE training, merging, and tokenization with hands-on code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMIs-gmxSac7"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2Rq2WZmSac8"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ How Does BPE Work? (Theory)\n",
        "\n",
        "Training steps:\n",
        "1. Normalize and pre-tokenize corpus into words.\n",
        "2. Build initial vocabulary from all characters in those words.\n",
        "3. Iteratively add new tokens by merging the most frequent pair of existing tokens.\n",
        "4. Repeat until vocab size is reached.\n",
        "\n",
        "This algorithm learns efficient subword merges so rare words get split but common ones are compact.\n"
      ],
      "metadata": {
        "id": "jTI6xXKGSv7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ BPE Training: An Example Corpus\n",
        "\n",
        "We'll use a toy corpus demonstrating how the base vocabulary and merges are built.\n"
      ],
      "metadata": {
        "id": "35L8bqblS93s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example toy corpus (simple sentences)\n",
        "corpus=[\n",
        "    \"This is the Hugging Face Course.\",\n",
        "    \"This chapter is about tokenization.\",\n",
        "    \"This section shows several tokenizer algorithms.\",\n",
        "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\"\n",
        "]"
      ],
      "metadata": {
        "id": "vM31vzk5TIQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Pre-Tokenization using GPT-2 Rules\n",
        "\n",
        "GPT-2 uses byte-level BPE and tracks spaces as special characters.\n",
        "We'll pre-tokenize all words using GPT-2's tokenizer.\n"
      ],
      "metadata": {
        "id": "lYWxQx4WTiOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from collections import defaultdict\n",
        "\n",
        "tokenizer=AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "word_freqs=defaultdict(int)\n",
        "\n",
        "for text in corpus:\n",
        "  words_with_offsets=tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "  new_words=[word for word,offset in words_with_offsets]\n",
        "  for word in new_words:\n",
        "    word_freqs[word]+=1\n",
        "print(word_freqs) # Shows frequency dict, e.g. {'This': 3, 'ƒ†is': 2, ...}"
      ],
      "metadata": {
        "id": "0VycuvglTefg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Build the Base Vocabulary\n",
        "\n",
        "Initialize the vocabulary with all unique characters found after normalization and pre-tokenization.\n"
      ],
      "metadata": {
        "id": "E9ajtDK7VdxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alphabet=[]\n",
        "for word in word_freqs.keys():\n",
        "  for letter in word:\n",
        "    if letter not in alphabet:\n",
        "      alphabet.append(letter)\n",
        "alphabet.sort()\n",
        "print(alphabet) # e.g. [',', '.', 'C', 'F', 'H', ..., 'ƒ†']"
      ],
      "metadata": {
        "id": "femwwlQHVqSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Prepare Token Splits for Training\n",
        "\n",
        "Split each word into its character list for merge tracking.\n"
      ],
      "metadata": {
        "id": "I-N9pSUXWP-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab=[\"<|endoftext|>\"]+alphabet.copy()\n",
        "splits={word:[c for c in word] for word in word_freqs.keys()}\n",
        "print(splits)"
      ],
      "metadata": {
        "id": "cQXEJfM7WYNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6Ô∏è‚É£ Pair Frequencies: Which Pairs to Merge?\n",
        "\n",
        "Define a helper function to count frequencies of each character pair in the current splits.\n"
      ],
      "metadata": {
        "id": "OOQieJPfXAMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_pair_freqs(splits):\n",
        "  pair_freqs=defaultdict(int)\n",
        "  for word,freq in word_freqs.items():\n",
        "    split=splits[word]\n",
        "    if len(split)==1:\n",
        "      continue\n",
        "    for i in range(len(split)-1):\n",
        "      pair=(split[i],split[i+1])\n",
        "      pair_freqs[pair]+=freq\n",
        "  return pair_freqs\n",
        "\n",
        "pair_freqs=compute_pair_freqs(splits)\n",
        "for i,key in enumerate(list(pair_freqs.keys())[:6]):\n",
        "  print(f\"{key}:{pair_freqs[key]}\")"
      ],
      "metadata": {
        "id": "wWy3qNyYXILi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7Ô∏è‚É£ Merge the Most Frequent Pair\n",
        "\n",
        "Find the pair with the max frequency, record the merge, and update splits and vocabulary.\n",
        "Repeat until reaching target vocab size.\n"
      ],
      "metadata": {
        "id": "BNeo-1_SYrCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_pair(a,b,splits):\n",
        "  for word in word_freqs:\n",
        "    split=splits[word] # Assign the value here\n",
        "    if len(split)==1:\n",
        "      continue\n",
        "    i=0\n",
        "    while i<len(split)-1:\n",
        "      if split[i]==a and split[i+1]==b:\n",
        "        split=split[:i]+[a+b]+split[i+2 :]\n",
        "      else:\n",
        "        i+=1\n",
        "    splits[word]=split\n",
        "  return splits\n",
        "\n",
        "# Example: First merge is ('G','t')\n",
        "splits=merge_pair('ƒ†', 't', splits)\n",
        "print(splits[\"ƒ†trained\"])"
      ],
      "metadata": {
        "id": "gv1RSfVGY7CC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8Ô∏è‚É£ BPE Training Loop: Build a Small Example Vocabulary\n",
        "\n",
        "Let‚Äôs loop and learn up to e.g. vocab size 50.\n"
      ],
      "metadata": {
        "id": "BXNBuA_9awQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merges={('ƒ†', 't'):'ƒ†t'}\n",
        "vocab.append('ƒ†t')\n",
        "\n",
        "vocab_size=50\n",
        "while len(vocab)<vocab_size:\n",
        "  pair_freqs=compute_pair_freqs(splits)\n",
        "  best_pair=\"\"\n",
        "  max_freq=None\n",
        "  for pair,freq in pair_freqs.items():\n",
        "    if max_freq is None or max_freq<freq:\n",
        "      best_pair=pair\n",
        "      max_freq=freq\n",
        "  splits=merge_pair(*best_pair,splits)\n",
        "  merges[best_pair]=best_pair[0]+best_pair[1]\n",
        "  vocab.append(best_pair[0]+best_pair[1])\n",
        "\n",
        "print(merges) # Learned merge rules\n",
        "print(vocab)  # All learned tokens\n"
      ],
      "metadata": {
        "id": "QHx1jTlLa1OE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9Ô∏è‚É£ Tokenization With Learned Rules\n",
        "\n",
        "Apply all merge rules, in order, to a new text to get its BPE tokenization.\n"
      ],
      "metadata": {
        "id": "ZQHpZGVFcXO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
        "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
        "    for pair, merge in merges.items():\n",
        "        for idx, split in enumerate(splits):\n",
        "            i = 0\n",
        "            while i < len(split) - 1:\n",
        "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
        "                    split = split[:i] + [merge] + split[i + 2 :]\n",
        "                else:\n",
        "                    i += 1\n",
        "            splits[idx] = split\n",
        "\n",
        "    return sum(splits, [])\n",
        "\n",
        "    # Try on a new sentence\n",
        "    print(tokenize(\"This is not a token\"))"
      ],
      "metadata": {
        "id": "8HGi3Ip3cceu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ Summary\n",
        "\n",
        "- **BPE** builds its vocabulary by greedy pairing and merging most frequent character pairs until target vocab size.\n",
        "- Allows rare/out-of-vocabulary words to be split while keeping common words as single tokens.\n",
        "- Real-world models use byte-level BPE (every possible byte is in vocab) for robustness.\n",
        "- Implementing BPE at small scale helps you understand how and why LLM tokenizers work!"
      ],
      "metadata": {
        "id": "wnih7AkJeFmU"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Byte-Pair Encoding tokenization",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}