{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esvLduKdPd8x"
      },
      "source": [
        "# ✨  Normalization and pre-tokenization\n",
        "Before subword tokenization (like BPE, WordPiece, or Unigram) takes place, inputs are **normalized** and **pre-tokenized**.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE5bkN2XPd8z"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bvs2UNCmPd8z"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1️⃣ Normalization: Cleaning Text Before Tokenization\n",
        "\n",
        "Normalization prepares text for tokenization by:\n",
        "- Lowercasing,\n",
        "- Removing accents,\n",
        "- Stripping redundant whitespace,\n",
        "- Possibly applying Unicode normalization (NFC/NFKC, etc)."
      ],
      "metadata": {
        "id": "p5Gv7AAkRG1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from  transformers import AutoTokenizer\n",
        "\n",
        "# Load a typical \"uncased\" BERT tokenizer,which applies normalization\n",
        "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Display the type of backend tokenization object(Hugging Face Tokenizers Library)\n",
        "print(type(tokenizer.backend_tokenizer))\n",
        "\n",
        "# Use the .normalizer.normalize_str method to see normalization directly\n",
        "example_text=\"Héllò hôw are ü?\"\n",
        "print(tokenizer.backend_tokenizer.normalizer.normalize_str(example_text)) # Should be \"hello how are u?\""
      ],
      "metadata": {
        "id": "W1nE6vXURGVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try: Normalization with a Cased Tokenizer\n",
        "\n",
        "Check how normalization works differently when you use cased vs. uncased models.\n"
      ],
      "metadata": {
        "id": "UlEBbkSOSf0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_cased=AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "print(tokenizer_cased.backend_tokenizer.normalizer.normalize_str(example_text))\n",
        "# \"Héllò hôw are ü?\" — different, as cased does NOT lowercase or remove accents!\n"
      ],
      "metadata": {
        "id": "iVhiXtoJSilY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2️⃣ Pre-Tokenization: First Splits (Before Subword Model Training or Tokenization)\n",
        "\n",
        "A tokenizer first splits text into chunks (\"pre-tokens\")—commonly words, punctuation, and spaces.  \n",
        "This step is **essential** before learning subword merges during training.\n"
      ],
      "metadata": {
        "id": "ZMVE8MgMTYM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For demonstration, use the same basic sentence for all tokenizers\n",
        "sample=\"Hello,how are  you?\"\n",
        "\n",
        "# BERT:splits into words & punctuation, ignores double spaces\n",
        "bert_tok=AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "print(bert_tok.backend_tokenizer.pre_tokenizer.pre_tokenize_str(sample))\n",
        "# [('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]"
      ],
      "metadata": {
        "id": "WQHyo12dTh8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3️⃣ Different Tokenizers, Different Pre-tokenization Rules\n",
        "\n",
        "### GPT-2: Whitespace + punctuation split, but keeps leading spaces! (using special marker Ġ)\n"
      ],
      "metadata": {
        "id": "WFWM-eh4UoWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_tok=AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "print(gpt2_tok.backend_tokenizer.pre_tokenizer.pre_tokenize_str(sample))\n",
        "# [('Hello', (0, 5)), (',', (5, 6)), ('Ġhow', (6, 10)), ('Ġare', (10, 14)), ('Ġ', (14, 15)), ('Ġyou', (15, 19)), ('?', (19, 20))]"
      ],
      "metadata": {
        "id": "xHV--E2wUxHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### T5 (SentencePiece): Splits on whitespace (special \"▁\") but not punctuation.\n",
        "\n",
        "Also, note the added space at the start!\n"
      ],
      "metadata": {
        "id": "m9TCGId_Wv6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t5_tok=AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "print(t5_tok.backend_tokenizer.pre_tokenizer.pre_tokenize_str(sample))\n",
        "# [('▁Hello,', (0, 6)), ('▁how', (7, 10)), ('▁are', (11, 14)), ('▁you?', (16, 20))]"
      ],
      "metadata": {
        "id": "EoZ5u-cVWzP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4️⃣ Why Is This Important? (Reversible Example)\n",
        "\n",
        "- BERT tokenizer removes repeating spaces (non-reversible).\n",
        "- GPT-2/SentencePiece encodes spaces as special characters, so decoding is \"reversible\": you can reconstruct *normalized* text, including spacing, from tokens.\n",
        "- Languages without explicit spaces (like Chinese or Japanese) benefit: SentencePiece does not rely on pre-tokenization.\n"
      ],
      "metadata": {
        "id": "GPI85YirXVCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Summary\n",
        "\n",
        "- **Normalization** = clean, standardize, or lowercase text\n",
        "- **Pre-tokenization** = split into word/punctuation \"pre-tokens\" + offsets\n",
        "- Different models (BERT, GPT-2, T5) apply different steps, yielding different results.\n",
        "- Understanding these steps is essential before building or customizing tokenizer pipelines for new domains or languages.\n",
        "\n",
        "Ready to explore how the three principal algorithms work? Onward to BPE, WordPiece, and Unigram!\n"
      ],
      "metadata": {
        "id": "OXnq5z48YBYB"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Normalization and pre-tokenization",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}