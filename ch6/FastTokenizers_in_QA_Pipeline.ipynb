{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lakshmi-Adhikari-AI/LLM-HuggingFace/blob/main/ch6/FastTokenizers_in_QA_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvjunN-CEXA7"
      },
      "source": [
        "# ü§ñ Fast tokenizers in the QA pipeline (PyTorch)\n",
        "\n",
        "Let's dig deep into how the Tokenizers library powers the QA pipeline‚Äîextracting answers using offset mapping, even with long contexts!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9VtYU6WEXA-"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnvQhgNIEXA-"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ Quick QA Using the Pipeline\n",
        "\n",
        "You can extract answers in a single call with the QA pipeline. Let's see how it works.\n"
      ],
      "metadata": {
        "id": "NP1vPy9TEuyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer=pipeline(\"question-answering\")\n",
        "context=\"\"\"\n",
        "ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch, and TensorFlow ‚Äî with a seamless integration\n",
        "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
        "\"\"\"\n",
        "question=\"which deep learing libraries back ü§ó Transformers? \"\n",
        "question_answerer(question=question,context=context)\n"
      ],
      "metadata": {
        "id": "lU8H8UMCEws-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ QA Works, Even With Long Contexts\n",
        "\n",
        "The pipeline can find answers even at the end of *very* long documents, thanks to chunking and offset handling.\n"
      ],
      "metadata": {
        "id": "LmLgvuBaF2lU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "long_context = \"\"\"\n",
        "ü§ó Transformers: State of the Art NLP\n",
        "ü§ó Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
        "question answering, summarization, translation, text generation and more in over 100 languages.\n",
        "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
        "ü§ó Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
        "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
        "can be modified to enable quick research experiments.\n",
        "Why should I use transformers?\n",
        "1. Easy-to-use state-of-the-art models:\n",
        "  - High performance on NLU and NLG tasks.\n",
        "  - Low barrier to entry for educators and practitioners.\n",
        "  - Few user-facing abstractions with just three classes to learn.\n",
        "  - A unified API for using all our pretrained models.\n",
        "  - Lower compute costs, smaller carbon footprint:\n",
        "2. Researchers can share trained models instead of always retraining.\n",
        "  - Practitioners can reduce compute time and production costs.\n",
        "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
        "3. Choose the right framework for every part of a model's lifetime:\n",
        "  - Train state-of-the-art models in 3 lines of code.\n",
        "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
        "  - Seamlessly pick the right framework for training, evaluation and production.\n",
        "4. Easily customize a model or an example to your needs:\n",
        "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
        "  - Model internals are exposed as consistently as possible.\n",
        "  - Model files can be used independently of the library for quick experiments.\n",
        "ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch and TensorFlow ‚Äî with a seamless integration\n",
        "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
        "\"\"\"\n",
        "\n",
        "question_answerer(question=question,context=long_context)\n"
      ],
      "metadata": {
        "id": "lofSp6r9F5UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Manual QA: Tokenizing and Model Forward Pass\n",
        "\n",
        "Let's reproduce the pipeline steps in detail (tokenization, masking, logits).\n"
      ],
      "metadata": {
        "id": "g-s81q3IGPW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer,AutoModelForQuestionAnswering\n",
        "\n",
        "model_checkpoint=\"distilbert-base-cased-distilled-squad\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model=AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "inputs=tokenizer(question,context,return_tensors=\"pt\")\n",
        "outputs=model(**inputs)\n",
        "start_logits=outputs.start_logits\n",
        "end_logits=outputs.end_logits\n",
        "\n",
        "\n",
        "print(start_logits.shape,end_logits.shape) #(1,num_tokens),(1,num_tokens)"
      ],
      "metadata": {
        "id": "MBpU23owGT4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Mask Out Irrelevant Tokens\n",
        "\n",
        "We only want to predict answer spans that are in the context‚Äînot in the question or [SEP] tokens.\n"
      ],
      "metadata": {
        "id": "VrrpRnkDHlnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "sequence_ids=inputs.sequence_ids() # maks out tokens not in context(sequence id1=context)\n",
        "mask=[i!=1 for i in sequence_ids]\n",
        "mask[0]=False                     # unmask [cls] token\n",
        "mask=torch.tensor(mask)[None]\n",
        "start_logits[mask]=-10000\n",
        "end_logits[mask]=-10000"
      ],
      "metadata": {
        "id": "hFMtuokBHqOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Convert Logits to Probabilities\n",
        "\n",
        "Softmax yields estimated start and end token probabilities.\n"
      ],
      "metadata": {
        "id": "PDf1EvU6In1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_probabilities=torch.nn.functional.softmax(start_logits,dim=-1)[0]\n",
        "end_probabilities=torch.nn.functional.softmax(end_logits,dim=-1)[0]"
      ],
      "metadata": {
        "id": "QGTTNv5PIsFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6Ô∏è‚É£ Find Best Start and End Indices\n",
        "\n",
        "Search for the (start, end) pair (with start ‚â§ end) with the highest probability.\n"
      ],
      "metadata": {
        "id": "ClWWk_B0JKwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = start_probabilities[:, None] * end_probabilities[None, :]\n",
        "scores = torch.triu(scores) # only keep scores for start <= end\n",
        "max_index = scores.argmax().item()\n",
        "start_index = max_index // scores.shape[1]\n",
        "end_index = max_index % scores.shape[1]\n",
        "print(scores[start_index, end_index])"
      ],
      "metadata": {
        "id": "02NFIaATJOJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7Ô∏è‚É£ Convert Token Indices to Character Spans (using Offsets)\n",
        "\n",
        "Map token-level indices back to the actual answer in the context, using offset mappings.\n"
      ],
      "metadata": {
        "id": "yfsbQLFCKQwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_with_offsets=tokenizer(question,context,return_offsets_mapping=True)\n",
        "offsets=inputs_with_offsets[\"offset_mapping\"]\n",
        "start_char,_=offsets[start_index]\n",
        "_,end_char=offsets[end_index]\n",
        "answer=context[start_char:end_char]\n",
        "result={\n",
        "    \"answer\":answer,\n",
        "    \"start\":start_char,\n",
        "    \"end\":end_char,\n",
        "    \"score\":scores[start_index,end_index].item(),\n",
        "\n",
        "}\n",
        "print(result)"
      ],
      "metadata": {
        "id": "GljSHZxhKTyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8Ô∏è‚É£ Handling Long Contexts: Stride Splitting\n",
        "\n",
        "Let's see what happens when the context is longer than the model's input length.\n"
      ],
      "metadata": {
        "id": "RuCwSHtKLPfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=tokenizer(question,long_context)\n",
        "print(len(inputs[\"input_ids\"])) # may be greater than 384"
      ],
      "metadata": {
        "id": "Ux8gR9yJLTCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9Ô∏è‚É£ Truncate the Context (\"only_second\" strategy)\n",
        "\n",
        "When needed, the model splits the context into overlapping windows (with stride).\n"
      ],
      "metadata": {
        "id": "klFPL5KoLmY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=tokenizer(question,long_context,max_length=384,truncation=\"only_second\")\n",
        "print(tokenizer.decode(inputs[\"input_ids\"]))"
      ],
      "metadata": {
        "id": "UVT60xLYLpXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOi4s3NGEXBF"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(question, long_context, max_length=384, truncation=\"only_second\")\n",
        "print(tokenizer.decode(inputs[\"input_ids\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîü Use Overflowing Tokens and Stride for Complete Coverage\n",
        "\n",
        "Automatically split long contexts for full answer search coverage.\n"
      ],
      "metadata": {
        "id": "AYGveQ7FMQP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"This sentence is not too long but we are going to split it anyway.\"\n",
        "inputs2=tokenizer(\n",
        "    sentence,truncation=True,return_overflowing_tokens=True,max_length=6,stride=2\n",
        ")\n",
        "\n",
        "for ids in inputs2[\"input_ids\"]:\n",
        "  print(tokenizer.decode(ids))\n",
        "\n",
        "print(inputs2.keys())\n",
        "print(inputs2[\"overflow_to_sample_mapping\"]) # shows which split come from which input"
      ],
      "metadata": {
        "id": "TbCVGYtNMUGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLxHGeGAEXBF"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"This sentence is not too long but we are going to split it anyway.\",\n",
        "    \"This sentence is shorter but will still get split.\",\n",
        "]\n",
        "inputs = tokenizer(\n",
        "    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n",
        ")\n",
        "\n",
        "print(inputs[\"overflow_to_sample_mapping\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£1Ô∏è‚É£ Apply to a Real Long QA Example\n",
        "\n",
        "Let‚Äôs split the long context and process all chunks‚Äîkeep stride and offset mappings for answer extraction.\n"
      ],
      "metadata": {
        "id": "SD342j7JNrii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=tokenizer(\n",
        "    question,\n",
        "    long_context,\n",
        "    stride=128,\n",
        "    max_length=384,\n",
        "    padding=\"longest\",\n",
        "    truncation=\"only_second\",\n",
        "    return_overflowing_tokens=True,\n",
        "    return_offsets_mapping=True\n",
        ")\n",
        "\n",
        "_=inputs.pop(\"overflow_to_sample_mapping\")\n",
        "offsets=inputs.pop(\"offset_mapping\")\n",
        "inputs=inputs.convert_to_tensors(\"pt\")\n",
        "print(inputs[\"input_ids\"].shape) # e.g (n_chunks,max_len)"
      ],
      "metadata": {
        "id": "-kOin_UANxz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£2Ô∏è‚É£ Model Outputs for Each Chunk\n",
        "\n",
        "The model processes all segments. Mask padding as well as non-context.\n"
      ],
      "metadata": {
        "id": "5N065ftQOq42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs=model(**inputs)\n",
        "start_logits=outputs.start_logits\n",
        "end_logits=outputs.end_logits\n",
        "print(start_logits.shape,end_logits.shape)\n",
        "\n",
        "sequence_ids=inputs.sequence_ids()\n",
        "mask=[i!=1 for i in sequence_ids]\n",
        "mask[0]=False\n",
        "mask=torch.logical_or(torch.tensor(mask)[None],inputs[\"attention_mask\"]==0)\n",
        "start_logits[mask]=-10000\n",
        "end_logits[mask]=-10000"
      ],
      "metadata": {
        "id": "WwRGxN_KOvC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£3Ô∏è‚É£ Collect Best Candidate Answers Across All Chunks\n",
        "\n",
        "Aggregate all best (start, end, score) tuples and convert token to character spans.\n"
      ],
      "metadata": {
        "id": "clvM9Q5FPmnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidates = []\n",
        "start_probs = torch.nn.functional.softmax(start_logits, dim=-1)\n",
        "end_probs = torch.nn.functional.softmax(end_logits, dim=-1)\n",
        "for start_prob, end_prob in zip(start_probs, end_probs):\n",
        "    scores = start_prob[:, None] * end_prob[None, :]\n",
        "    idx = torch.triu(scores).argmax().item()\n",
        "    start_idx = idx // scores.shape[1]\n",
        "    end_idx = idx % scores.shape[1]\n",
        "    score = scores[start_idx, end_idx].item()\n",
        "    candidates.append((start_idx, end_idx, score))\n",
        "print(candidates)\n",
        "\n",
        "# Now map each candidate to its charecter span in the context\n",
        "\n",
        "for candidate, offset in zip(candidates, offsets):\n",
        "    start_token, end_token, score = candidate\n",
        "    start_char, _ = offset[start_token]\n",
        "    _, end_char = offset[end_token]\n",
        "    answer = long_context[start_char:end_char]\n",
        "    result = {\"answer\": answer, \"start\": start_char, \"end\": end_char, \"score\": score}\n",
        "    print(result)"
      ],
      "metadata": {
        "id": "bVx6AR0jPyJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ Summary\n",
        "\n",
        "Fast tokenizers make QA possible on arbitrarily long texts, using offset mapping, overflowing tokens, and context splitting‚Äîfeatures vital for robust question answering in modern NLP.\n"
      ],
      "metadata": {
        "id": "Dv2QBNwrSM1O"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Fast tokenizers in the QA pipeline (PyTorch)",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}