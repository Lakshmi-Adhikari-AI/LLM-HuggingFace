{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lakshmi-Adhikari-AI/LLM-HuggingFace/blob/main/ch2/mod3_tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLQdoW4U3Lr4"
      },
      "source": [
        "# Tokenizers (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Irpv2YYP3Lr7"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "HHLJhySV3Lr8",
        "outputId": "c2b7c00a-0fc8-466e-d719-636d4929c5ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.4)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (5.29.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tokenization: Converting text into subparts (chunks) like words, subwords, or characters.**\n",
        "\n"
      ],
      "metadata": {
        "id": "6a36tc3O-rMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üîπ Types of Tokenization**\n",
        "**1. Word-based Tokenization**\n",
        "\n",
        "\n",
        "*   Splits text by space or punctuation.\n",
        "*   Easy to understand.\n",
        "*   Fails for new words (e.g., ‚Äúdogs‚Äù ‚â† ‚Äúdog‚Äù) and requires a large vocabulary.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nxWwu8gOCqUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word-based Tokenization using space\n",
        "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
        "print(tokenized_text)\n"
      ],
      "metadata": {
        "id": "DGC_86S93SaQ",
        "outputId": "a280d286-44ec-4772-c99c-15c9117c5e24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word-based Tokenization using punctuation (via NLTK)\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "tokenizer = WordPunctTokenizer()\n",
        "tokenized_text = tokenizer.tokenize(\"Let's do Tokenization!\")\n",
        "print(tokenized_text)\n"
      ],
      "metadata": {
        "id": "wAF3Ino43SHA",
        "outputId": "a6f3f26e-e880-4b4b-ac3c-e95309f25109",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Let', \"'\", 's', 'do', 'Tokenization', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Character-based Tokenization**\n",
        "\n",
        "* Splits text into individual characters.\n",
        "\n",
        "* Low vocabulary size, handles unknown words.\n",
        "\n",
        "* Less meaningful as single characters carry little meaning."
      ],
      "metadata": {
        "id": "12BX-QbmDWzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Character-based Tokenization\n",
        "tokenized_text = list(\"Let's do Tokenization!\")\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "id": "EU9ERZWY3nbW",
        "outputId": "251ca2f5-b217-4361-e987-63990f024667",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['L', 'e', 't', \"'\", 's', ' ', 'd', 'o', ' ', 'T', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Subword-based Tokenization**\n",
        "\n",
        "* Breaks rare/long words into smaller parts.\n",
        "\n",
        "* Balances vocabulary size and flexibility.\n",
        "\n",
        "* Most commonly used in models like BERT, GPT."
      ],
      "metadata": {
        "id": "sjlEAdiZDlkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subword-based Tokenization using BertTokenizer\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "tokenized_text = tokenizer.tokenize(\"Let's do Tokenization!\")\n",
        "print(tokenized_text)\n"
      ],
      "metadata": {
        "id": "EHDe-4t-3qWq",
        "outputId": "b047ccff-9eb7-4600-ecbf-19fd0664a52d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Let', \"'\", 's', 'do', 'To', '##ken', '##ization', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Subword-based Tokenization using AutoTokenizer (auto-selects correct tokenizer based on checkpoint)\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "tokenized_text = tokenizer.tokenize(\"Let's do Tokenization!\")\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "id": "7cS66JjN3yUG",
        "outputId": "98e150dc-59c7-40fb-d2b8-23816db36c2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Let', \"'\", 's', 'do', 'To', '##ken', '##ization', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üîπ Encoding: Converting Text into Numbers**\n",
        "\n",
        "**Encoding has 2 steps:**\n",
        "\n",
        "1. Tokenization ‚Üí Convert text to tokens\n",
        "\n",
        "2. Token ID Conversion ‚Üí Convert tokens to vocabulary IDs"
      ],
      "metadata": {
        "id": "E4g8Q_fS36AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Tokenize the text\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "input_tokens = tokenizer.tokenize(\"Using a Transformer network is simple\")\n",
        "print(input_tokens)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6uYynHxf31Js",
        "outputId": "11ee3cba-46d2-42a2-bf29-2ed2d1f149cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Convert tokens to token IDs\n",
        "token_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
        "print(token_ids)\n"
      ],
      "metadata": {
        "id": "YL6Perrm303_",
        "outputId": "8bc95d43-a956-4761-c2f4-eedc8ad6e7b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîé Check token presence in vocabulary**"
      ],
      "metadata": {
        "id": "iljT02AGEdLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Does 'Transformer' exist in vocab?\", tokenizer.vocab.get(\"Transformer\"))\n",
        "print(\"Does 'transform' exist?\", tokenizer.vocab.get(\"transform\"))\n",
        "print(\"Does 'Trans' exist?\", tokenizer.vocab.get(\"Trans\"))\n",
        "print(\"Does '##former' exist?\", tokenizer.vocab.get(\"##former\"))"
      ],
      "metadata": {
        "id": "HI5WQNp9EhE9",
        "outputId": "dae087c1-1536-4cc9-cf15-bf179ce6cab3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Does 'Transformer' exist in vocab? None\n",
            "Does 'transform' exist? 11303\n",
            "Does 'Trans' exist? 13809\n",
            "Does '##former' exist? 23763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîÑ Decoding: Converting IDs back to human-readable text**"
      ],
      "metadata": {
        "id": "E1pibwICEoon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode - get back complete words of the rawtext from the token_ids\n",
        "\n",
        "decode_text=tokenizer.decode(token_ids)\n",
        "print(decode_text)"
      ],
      "metadata": {
        "id": "-9C9q-7S8g5B",
        "outputId": "ebcff53d-91bc-4f0d-9208-e5779271aa3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using a Transformer network is simple\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìå Manually Add Special Tokens ([CLS], [SEP])**"
      ],
      "metadata": {
        "id": "MATke7oiEvi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In direct tokenizer based on the checkpoint it add these specials tokens\n",
        "\n",
        "tokens=[tokenizer.cls_token] + [tokenizer.tokenize(\"Using a Transformer network is simple\")] + [tokenizer.sep_token]\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "Om4pClcq9tUx",
        "outputId": "e5825bf1-94ba-46f7-b247-2b5dc4abde82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', ['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple'], '[SEP]']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Tokenizers (PyTorch)",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}