{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6240a6f",
   "metadata": {},
   "source": [
    "## üìò tokenizer() ‚Äì One Call for Everything\n",
    "\n",
    "The `tokenizer()` method can handle all the steps in one go:\n",
    "\n",
    "- Tokenizing text\n",
    "- Converting tokens to IDs\n",
    "- Padding and truncating\n",
    "- Creating attention masks\n",
    "- Preparing tensors to feed into the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441675b3",
   "metadata": {},
   "source": [
    "## ‚úÖ Tokenizer Object\n",
    "\n",
    "The tokenizer object knows everything: how to split tokens, convert to IDs, add padding, truncation, and attention masks.\n",
    "\n",
    "It returns a Python dictionary with `input_ids` and `attention_mask` when we give it sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21f51bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "print(tokenizer)\n",
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3bef84",
   "metadata": {},
   "source": [
    "## üßæ Single Sequence Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baada3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs)\n",
    "print(model_inputs.keys())\n",
    "print(\"Input_IDS:\", model_inputs['input_ids'])\n",
    "print(\"Attention_mask\", model_inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea64f31",
   "metadata": {},
   "source": [
    "## üì¶ Multiple Sequences (Batch Input)\n",
    "\n",
    "Tokenizer can process multiple sequences at once by passing them as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e23eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "model_inputs = tokenizer(sequences)\n",
    "print(model_inputs)\n",
    "print(\"Input_IDS:\", model_inputs['input_ids'])\n",
    "print(\"Attention_mask\", model_inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ff685",
   "metadata": {},
   "source": [
    "## üßä Padding Options\n",
    "\n",
    "- `\"longest\"` ‚Üí pad up to the longest sentence in the batch\n",
    "- `\"max_length\"` ‚Üí pad up to the model‚Äôs maximum length (e.g. 512)\n",
    "- `max_length=<int>` ‚Üí custom length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "print(model_inputs['input_ids'])\n",
    "\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "print(model_inputs['input_ids'])\n",
    "\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)\n",
    "print(model_inputs['input_ids'])\n",
    "\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8, truncation=True)\n",
    "print(model_inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cf994e",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Truncation Options\n",
    "\n",
    "- `truncation=True` ‚Üí truncate beyond model‚Äôs max (e.g. 512)\n",
    "- `max_length=<int>` ‚Üí custom truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5a297",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "print(model_inputs['input_ids'])\n",
    "\n",
    "model_inputs = tokenizer(sequences, truncation=True, max_length=8)\n",
    "print(model_inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8efb6bf",
   "metadata": {},
   "source": [
    "## üîπ Return Tensors for PyTorch / NumPy / TensorFlow\n",
    "\n",
    "- `return_tensors=\"pt\"` ‚Üí PyTorch\n",
    "- `return_tensors=\"np\"` ‚Üí NumPy\n",
    "- `return_tensors=\"tf\"` ‚Üí TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5d67c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "print(model_inputs)\n",
    "\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d58d9ed",
   "metadata": {},
   "source": [
    "## ‚úÖ Special Tokens in Transformers\n",
    "\n",
    "When using `tokenizer(sequence)` directly, it adds special tokens:\n",
    "\n",
    "- `[CLS]` ‚Äì Classification token at the start\n",
    "- `[SEP]` ‚Äì Separator token at the end\n",
    "\n",
    "These are required by models like BERT/DistilBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232ee65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs)\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n",
    "print(tokenizer.decode(model_inputs['input_ids']))\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c4855",
   "metadata": {},
   "source": [
    "## ‚úÖ Wrapping up: From Tokenizer to Model\n",
    "\n",
    "Now that we‚Äôve seen all the individual steps the tokenizer object uses when applied on texts, let‚Äôs see one final time how it can handle multiple sequences (padding!), very long sequences (truncation!), and multiple types of tensors with its main API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1cea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"So have I!\"\n",
    "]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "print(output.logits)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
