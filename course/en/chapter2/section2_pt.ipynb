{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lakshmi-Adhikari-AI/LLM-HuggingFace/blob/main/course/en/chapter2/section2_pt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm-tDv3T--Oq"
      },
      "source": [
        "# Behind the pipeline (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5QIttrK--Or"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "90nfxVtu--Os",
        "outputId": "b5d05c81-f273-4ee7-e1e3-445e2561c7b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.4)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (5.29.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "collapsed": true,
        "id": "3OSF-yKM--Ot",
        "outputId": "5441b3ea-287f-49d7-94cc-4c8d112668e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9998302459716797},\n",
              " {'label': 'NEGATIVE', 'score': 0.9895736575126648}]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "#  Pipeline approach - handles preprocessing, model inference, and postprocessing\n",
        "from transformers import pipeline\n",
        "\n",
        "# Sentiment analysis pipeline using default pretrained model\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Predict sentiment for two input sentences\n",
        "classifier(\n",
        "    [\n",
        "        \"I love AI\",\n",
        "        \"But it's complex so don't want to learn\"\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DLii2Bco--Ot"
      },
      "outputs": [],
      "source": [
        "#  AutoTokenizer - automatically selects correct tokenizer for the given model checkpoint\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Raw input sentences to be passed through the model\n",
        "raw_inputs = [\n",
        "    \"I love AI\",\n",
        "    \"But it's complex so don't want to learn\"\n",
        "]\n",
        "\n",
        "# Tokenizing the raw inputs: adds [CLS], [SEP], and converts to token IDs\n",
        "# padding=True → ensures all inputs are same length\n",
        "# truncation=True → trims tokens if they exceed model's max length\n",
        "# return_tensors=\"pt\" → returns PyTorch tensors\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "collapsed": true,
        "id": "kp7BAGbs--Ou",
        "outputId": "43f0105d-528f-407e-b679-a1079260e80f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 5.3020e-01,  2.4128e-01,  6.9855e-02,  ...,  4.2687e-01,\n",
            "           1.0070e+00, -4.3037e-01],\n",
            "         [ 6.6918e-01,  4.0507e-01,  6.9381e-02,  ...,  4.8217e-01,\n",
            "           9.4859e-01, -2.7963e-01],\n",
            "         [ 9.1037e-01,  4.6725e-01,  3.0641e-01,  ...,  3.7685e-01,\n",
            "           9.2645e-01, -2.9975e-01],\n",
            "         ...,\n",
            "         [ 2.2623e-01,  2.0512e-01, -6.8777e-02,  ...,  4.6769e-01,\n",
            "           9.5997e-01, -3.5636e-01],\n",
            "         [ 2.0195e-01,  2.8293e-01, -3.0646e-04,  ...,  4.7270e-01,\n",
            "           8.5636e-01, -3.9502e-01],\n",
            "         [ 1.8929e-01,  1.6243e-01, -8.3447e-02,  ...,  5.0290e-01,\n",
            "           9.6060e-01, -3.6688e-01]],\n",
            "\n",
            "        [[-1.2220e-01,  5.3712e-01,  1.6431e-02,  ..., -2.6850e-01,\n",
            "           2.0008e-01,  1.7445e-01],\n",
            "         [-5.3642e-02,  3.3243e-01, -2.8873e-02,  ..., -2.8136e-01,\n",
            "           3.1022e-01,  5.7603e-01],\n",
            "         [-3.5764e-01,  6.1211e-01, -1.7822e-02,  ..., -5.3864e-01,\n",
            "           1.4982e-01,  2.2937e-01],\n",
            "         ...,\n",
            "         [-9.8159e-02,  4.7066e-01,  2.3455e-01,  ..., -2.4201e-01,\n",
            "          -2.8057e-01,  1.5981e-01],\n",
            "         [-2.5857e-01,  6.8563e-01, -7.7932e-02,  ..., -2.9707e-01,\n",
            "          -7.2680e-02, -1.7702e-01],\n",
            "         [ 5.7695e-02,  7.4519e-01,  8.7815e-02,  ..., -2.7883e-01,\n",
            "          -1.8722e-01, -2.2545e-01]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "torch.Size([2, 13, 768])\n"
          ]
        }
      ],
      "source": [
        "#  AutoModel - loads only the base Transformer model (no classification head)\n",
        "from transformers import AutoModel\n",
        "\n",
        "# Load pretrained model checkpoint (only base transformer, not task-specific head)\n",
        "model = AutoModel.from_pretrained(checkpoint)\n",
        "\n",
        "# Run model on inputs → returns last_hidden_state (token embeddings)\n",
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state)\n",
        "print(outputs.last_hidden_state.shape)  # e.g., torch.Size([2, 16, 768]) → [batch_size, seq_length, hidden_size]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "GIo6FJLL--Ou",
        "outputId": "dfce6816-3e92-402d-9aba-a75cbc124a3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2])\n",
            "tensor([[-4.1860,  4.4950],\n",
            "        [ 1.3378, -1.1082]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#  AutoModelForSequenceClassification - includes classification head on top of base model\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Load model with classification head for sentiment analysis task\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "# Run model to get raw predictions (logits)\n",
        "outputs = model(**inputs)\n",
        "print(outputs.logits.shape)  # e.g., torch.Size([2, 2]) → 2 sentences, 2 sentiment classes (NEGATIVE, POSITIVE)\n",
        "print(outputs.logits)        # Raw scores before softmax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "collapsed": true,
        "id": "UDVNLiQ8--Ou",
        "outputId": "e57b415e-ea54-458c-a1df-1beded22ead0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.6975e-04, 9.9983e-01],\n",
            "        [9.2027e-01, 7.9729e-02]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.0000, 1.0000],\n",
            "        [0.9200, 0.0800]], grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#  Convert logits (raw scores) to probabilities using softmax\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "predictions = F.softmax(outputs.logits, dim=1)\n",
        "print(predictions)  # Each row sums to 1 → [NEGATIVE_prob, POSITIVE_prob]\n",
        "print(torch.round(predictions * 100) / 100)  # Optional: Round to 2 decimal places for easier reading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "NDIV9P5k--Ou",
        "outputId": "bafc991a-c926-45c3-bb79-48e0787c4b3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 0])\n",
            "['POSITIVE', 'NEGATIVE']\n",
            "Sentence 1: POSITIVE\n",
            "Sentence 2: NEGATIVE\n"
          ]
        }
      ],
      "source": [
        "#  Extract predicted label indices\n",
        "highest_indices = predictions.argmax(dim=1)  # Returns index of max probability (0 or 1)\n",
        "print(highest_indices)\n",
        "\n",
        "#  Map predicted indices to actual label names using model config\n",
        "labels = [model.config.id2label[idx.item()] for idx in highest_indices]\n",
        "print(labels)\n",
        "\n",
        "#  Print results in human-readable format\n",
        "for i, label in enumerate(labels):\n",
        "    print(f\"Sentence {i+1}: {label}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Behind the pipeline (PyTorch)",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}