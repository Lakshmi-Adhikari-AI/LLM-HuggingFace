# 🤖 LLM HuggingFace Course – Learning & Hands-on Practice

Welcome to my personal learning repository for the **Hugging Face Transformers course** on Large Language Models (LLMs)! 🚀

This repo captures my **step-by-step learning journey**, **hands-on code experiments**, and growing understanding of how to build and use modern NLP applications with transformer models. My goal is to gain strong practical experience and move confidently into the world of **Generative AI**.

---

## 📚 What You'll Find Here

This repository includes:

- 🧠 **Well-organized notebooks & `.py` scripts** for each chapter and module
- 🧪 **Hands-on experiments** with tokenization, encoding, model loading, inference, and more
- 🧾 **Colab notebooks** saved with real practice outputs
- 📘 **Personal notes** to reinforce learning and experimentation

I’ve broken down every concept, implemented examples, and explained them in my own words to solidify understanding — this is more than just running code, it’s about **mastering the fundamentals of LLMs**.

---




## 🧠 Topics Covered

- ✅ Introduction to the 🤗 Transformers Library  
- ✅ Using `pipeline()` for sentiment analysis and classification  
- ✅ Word-based, character-based, and subword-based **tokenization**  
- ✅ **Encoding & decoding**: converting text to input IDs and back  
- ✅ Handling **unknown tokens**, vocabulary lookups, and special tokens  
- ✅ **Loading pretrained models and tokenizers** using `AutoModel`, `AutoTokenizer`  
- ✅ Inference with attention masks, padding, truncation  
- ✅ Manual work with token IDs, logits, and softmax scoring
- ✅ **Handling multiple sequences** with batching, padding, and attention masks
- ✅ Putting It All Together – tokenizer + padding + truncation + tensors + model
- ✅ Optimized Inference Deployment – Overview of deploying LLMs using TGI, vLLM, and llama.cpp with memory optimization, advanced sampling, and cloud access
- ✅ **Chapter 2: "Using Transformers" – Completed ✅ (including the end-of-chapter quiz)**  

---

## 🔧 Tools & Libraries Used

- [Transformers](https://github.com/huggingface/transformers) by Hugging Face  
- Google Colab  
- PyTorch backend  
- NLTK for classic tokenization  
- GitHub for version control & tracking my progress

---

## 💡 My Learning Approach

- 💬 Understand each topic with simple explanations  
- 🔍 Break down and comment every code block  
- ✍️ Write scripts from scratch to reinforce logic  
- ✅ Verify outputs, tweak inputs, and test edge cases  
- 📒 Document everything clearly for future reference

---

## 🚀 Why This Repo?

This repository is a reflection of:

- My **commitment to mastering LLMs**
- My **excitement for Generative AI**
- My intent to **build real-world applications**
- My desire to **grow into a confident and capable GenAI developer**

If you're learning LLMs or exploring Hugging Face, feel free to check out the code, run the notebooks, and reach out if you’d like to collaborate or discuss!

---




