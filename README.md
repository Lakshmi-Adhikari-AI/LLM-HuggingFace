# ğŸ¤– LLM HuggingFace Course â€“ Learning & Hands-on Practice

Welcome to my personal learning repository for the **Hugging Face Transformers course** on Large Language Models (LLMs)! ğŸš€

This repo captures my **step-by-step learning journey**, **hands-on code experiments**, and growing understanding of how to build and use modern NLP applications with transformer models. My goal is to gain strong practical experience and move confidently into the world of **Generative AI**.

---

## ğŸ“š What You'll Find Here

This repository includes:

- ğŸ§  **Well-organized notebooks & `.py` scripts** for each chapter and module
- ğŸ§ª **Hands-on experiments** with tokenization, encoding, model loading, inference, and more
- ğŸ§¾ **Colab notebooks** saved with real practice outputs
- ğŸ“˜ **Personal notes** to reinforce learning and experimentation

Iâ€™ve broken down every concept, implemented examples, and explained them in my own words to solidify understanding â€” this is more than just running code, itâ€™s about **mastering the fundamentals of LLMs**.

---




## ğŸ§  Topics Covered

- âœ… Introduction to the ğŸ¤— Transformers Library  
- âœ… Using `pipeline()` for sentiment analysis and classification  
- âœ… Word-based, character-based, and subword-based **tokenization**  
- âœ… **Encoding & decoding**: converting text to input IDs and back  
- âœ… Handling **unknown tokens**, vocabulary lookups, and special tokens  
- âœ… **Loading pretrained models and tokenizers** using `AutoModel`, `AutoTokenizer`  
- âœ… Inference with attention masks, padding, truncation  
- âœ… Manual work with token IDs, logits, and softmax scoring
- âœ… **Handling multiple sequences** with batching, padding, and attention masks
- âœ… Putting It All Together â€“ tokenizer + padding + truncation + tensors + model
- âœ… Optimized Inference Deployment â€“ Overview of deploying LLMs using TGI, vLLM, and llama.cpp with memory optimization, advanced sampling, and cloud access
- âœ… **Chapter 2: "Using Transformers" â€“ Completed âœ… (including the end-of-chapter quiz)**  

---

## ğŸ”§ Tools & Libraries Used

- [Transformers](https://github.com/huggingface/transformers) by Hugging Face  
- Google Colab  
- PyTorch backend  
- NLTK for classic tokenization  
- GitHub for version control & tracking my progress

---

## ğŸ’¡ My Learning Approach

- ğŸ’¬ Understand each topic with simple explanations  
- ğŸ” Break down and comment every code block  
- âœï¸ Write scripts from scratch to reinforce logic  
- âœ… Verify outputs, tweak inputs, and test edge cases  
- ğŸ“’ Document everything clearly for future reference

---

## ğŸš€ Why This Repo?

This repository is a reflection of:

- My **commitment to mastering LLMs**
- My **excitement for Generative AI**
- My intent to **build real-world applications**
- My desire to **grow into a confident and capable GenAI developer**

If you're learning LLMs or exploring Hugging Face, feel free to check out the code, run the notebooks, and reach out if youâ€™d like to collaborate or discuss!

---




