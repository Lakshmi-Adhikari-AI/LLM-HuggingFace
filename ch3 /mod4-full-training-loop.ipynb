{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lakshmi-Adhikari-AI/LLM-HuggingFace/blob/main/ch3%20/mod4-full-training-loop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPVKdq66U_SF"
      },
      "source": [
        "# A full training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY_aixQIU_SG"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yczCdWfwU_SH"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the following line:\n",
        "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 3: Fine-Tuning from Scratch with a Custom Training Loop\n",
        "\n",
        "In this notebook, we implement the full training loop for fine-tuning a BERT model on the MRPC task **without using the HuggingFace `Trainer` API**.\n",
        "\n",
        "This approach gives us complete control over the training process, from data preparation and batching to loss computation, backpropagation, and evaluation.\n",
        "\n",
        "We will also see how to evaluate model performance on the validation dataset after training.\n"
      ],
      "metadata": {
        "id": "9jMEX95oVWii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß© Data Preparation\n",
        "\n",
        "- Load the GLUE MRPC dataset.\n",
        "- Use the BERT tokenizer to process sentence pairs.\n",
        "- Clean and format the dataset for PyTorch with tokenized inputs.\n",
        "- Prepare data collators to dynamically pad batches.\n"
      ],
      "metadata": {
        "id": "vV6knBY6VWb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "#  Load the MRPC dataset (sentence pairs with labels)\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "\n",
        "#  Load the tokenizer matching the pretrained BERT base model\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "#  Function to tokenize pairs of sentences, applying truncation to fit model max length\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "#  Tokenize the entire dataset efficiently in batched mode\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "\n",
        "#  Prepare the data collator that dynamically pads batches for efficient processing\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "#  Clean dataset: remove raw text columns, rename 'label' to 'labels', and set format to PyTorch tensors\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets.set_format(\"torch\")\n"
      ],
      "metadata": {
        "id": "9TCQ3FfnVdTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõçÔ∏è DataLoader Setup\n",
        "\n",
        "- Create PyTorch dataloaders for training and validation.\n",
        "- Shuffle training data each epoch for robust training.\n",
        "- Use the data collator for dynamic padding of batches.\n",
        "- Validate input batch shapes for correctness.\n"
      ],
      "metadata": {
        "id": "cbLmw_n8VfpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#  Create the training dataloader with shuffling enabled and batch size 8\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
        ")\n",
        "\n",
        "#  Create the validation dataloader with batch size 8 and no shuffling\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
        ")\n",
        "\n",
        "#  Inspect a batch to confirm shapes\n",
        "for batch in train_dataloader:\n",
        "    print({k: v.shape for k, v in batch.items()})\n",
        "    break\n"
      ],
      "metadata": {
        "id": "qqPretXZVhMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è Model, Optimizer, and Scheduler Setup\n",
        "\n",
        "- Load pretrained BERT with classification head for two labels.\n",
        "- Use AdamW optimizer with weight decay for stable training.\n",
        "- Set a linear learning rate scheduler covering all training steps.\n",
        "- Configure total epochs for training.\n"
      ],
      "metadata": {
        "id": "zf2EnC0OVi9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, get_scheduler\n",
        "from torch.optim import AdamW\n",
        "\n",
        "#  Load BERT model for sequence classification with 2 output labels\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "#  Initialize AdamW optimizer with learning rate 5e-5\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "#  Define total training steps (epochs √ó steps per epoch)\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "\n",
        "#  Setup learning rate scheduler with linear decay and no warmup\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n"
      ],
      "metadata": {
        "id": "8vBGxk6KVlh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíª Device Setup and Progress Bar\n",
        "\n",
        "- Detect GPU availability and move model accordingly.\n",
        "- Create progress bar for visual feedback on training progress.\n"
      ],
      "metadata": {
        "id": "fQyfdc_MVmYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "#  Select GPU if available, otherwise CPU\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "#  Initialize a progress bar for all training steps across epochs\n",
        "progress_bar = tqdm(range(num_training_steps))\n"
      ],
      "metadata": {
        "id": "twKdoCkzVoY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÑ Training Loop\n",
        "\n",
        "- Loop over epochs and batches.\n",
        "- Send batch to the correct device.\n",
        "- Forward pass to compute outputs and loss.\n",
        "- Backpropagation to compute gradients.\n",
        "- Optimizer step to update parameters.\n",
        "- Scheduler step to update learning rate.\n",
        "- Zero gradients before next step.\n",
        "- Update progress bar for visualization.\n"
      ],
      "metadata": {
        "id": "H8rPLJrSVrch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        #  Move batch data to device (CPU/GPU)\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        #  Forward pass: compute model outputs and loss\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        #  Backpropagation: compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        #  Optimizer step: update model weights\n",
        "        optimizer.step()\n",
        "\n",
        "        #  Scheduler step: decay learning rate\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        #  Zero gradients before next iteration\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #  Update progress bar\n",
        "        progress_bar.update(1)\n"
      ],
      "metadata": {
        "id": "eheWwb4uVtnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Evaluation Loop\n",
        "\n",
        "- Switch model to evaluation mode.\n",
        "- Loop over validation batches without gradient calculation.\n",
        "- Compute logits and predicted classes.\n",
        "- Collect all predictions and true labels for metric computation.\n",
        "- Use ü§ó Evaluate library to compute accuracy and F1.\n"
      ],
      "metadata": {
        "id": "Q5RfTIbDVxB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "#  Load GLUE MRPC evaluation metric (accuracy and F1)\n",
        "metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "\n",
        "model.eval()  # Switch model to evaluation mode\n",
        "\n",
        "for batch in eval_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "\n",
        "    #  Get the predicted class indices by selecting max logit\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    #  Add predictions and references for metric calculation\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "#  Compute final metrics\n",
        "results = metric.compute()\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "kwBPtrSaVy6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ö° Supercharge Your Training Loop with ü§ó Accelerate\n",
        "\n",
        "- Accelerate abstracts away device management and distributed training complexities.\n",
        "- Enables mixed precision training (fp16) and runs seamlessly on CPUs, GPUs, TPUs.\n",
        "- Allows you to keep writing familiar PyTorch training loops with minimal changes.\n"
      ],
      "metadata": {
        "id": "iVjddtnOV1A-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup Accelerate and Prepare Components"
      ],
      "metadata": {
        "id": "Oix5ih6KV9K5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "#  Initialize the accelerator to manage device and distributed setup\n",
        "accelerator = Accelerator()\n",
        "\n",
        "#  Load classification model as before\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "#  Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
        "\n",
        "#  Prepare dataloaders, model & optimizer with accelerator for distributed/mixed precision compatibility\n",
        "train_dl, eval_dl, model, optimizer = accelerator.prepare(\n",
        "    train_dataloader, eval_dataloader, model, optimizer\n",
        ")\n"
      ],
      "metadata": {
        "id": "9p8ksX0SV2rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scheduler and Progress Bar for Accelerate Loop"
      ],
      "metadata": {
        "id": "_zALu2azV-Ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_scheduler\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dl)\n",
        "\n",
        "#  Set up linear LR scheduler\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "\n",
        "#  Create progress bar for training steps\n",
        "progress_bar = tqdm(range(num_training_steps))\n"
      ],
      "metadata": {
        "id": "9tpQFjZPWBr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop Using Accelerate\n",
        "python"
      ],
      "metadata": {
        "id": "1bu2LVhmWF6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dl:\n",
        "        #  Forward pass & loss computation\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        #  Backpropagation using accelerator (supporting mixed precision etc.)\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        #  Update optimizer parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        #  Update learning rate\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        #  Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #  Update progress bar\n",
        "        progress_bar.update(1)\n"
      ],
      "metadata": {
        "id": "nkV83rlnWGkI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "A full training",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}