{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2tnWt1191JU"
      },
      "source": [
        "# üè∑Ô∏è Token Classification with Transformers (NER, POS, Chunking)\n",
        "\n",
        "Token classification attributes a label to **each token** in a sequence (e.g., NER, POS, Chunking).  \n",
        "We'll fine-tune BERT for NER using the CoNLL-2003 dataset.  \n",
        "Let's walk through from data loading to model training and inference!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaNOzBBx91JW"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C5ldiz_91JW"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the following line:\n",
        "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets==3.6.0 --force-reinstall\n"
      ],
      "metadata": {
        "id": "Wx7N-4oq_6ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YPPxUAR91JX"
      },
      "source": [
        "You will need to setup git, adapt your email and name in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF6iur5K91JX"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"\"\n",
        "!git config --global user.name \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp3mzxL991JY"
      },
      "source": [
        "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khclJwxf91JY"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ Load and Inspect the CoNLL-2003 Dataset\n",
        "\n",
        "News stories pre-tokenized into words, with multiple label columns.\n"
      ],
      "metadata": {
        "id": "IpgVy8r8-5e9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets=load_dataset(\"conll2003\")\n",
        "print(raw_datasets)"
      ],
      "metadata": {
        "id": "4Fv18zf4-9yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Visualize Token and Label Mappings\n",
        "\n",
        "Get tokens and integer NER labels for sample sentences, map them to class names.\n"
      ],
      "metadata": {
        "id": "G8aapbzNAdv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect first training example: tokens and NER tags\n",
        "print(\"Tokens:\",raw_datasets[\"train\"][0][\"tokens\"])\n",
        "print(\"NER tag IDs:\",raw_datasets[\"train\"][0][\"ner_tags\"])\n",
        "\n",
        "ner_feature=raw_datasets[\"train\"].features[\"ner_tags\"]\n",
        "label_names=ner_feature.feature.names\n",
        "print(\"NER Classes:\",label_names)"
      ],
      "metadata": {
        "id": "1DrEI4jIAiq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display word-label table for the first sentence\n",
        "words,labels=raw_datasets[\"train\"][0][\"tokens\"],raw_datasets[\"train\"][0][\"ner_tags\"]\n",
        "output_word=\"\"\n",
        "output_label=\"\"\n",
        "for w,l in zip(words,labels):\n",
        "  name=label_names[l]\n",
        "  length=max(len(w),len(name))\n",
        "  output_word += w+\" \"*(length-len(w)+1)\n",
        "  output_label += name+\" \"*(length-len(name)+1)\n",
        "print(output_word)\n",
        "print(output_label)"
      ],
      "metadata": {
        "id": "vvhYae0rCDpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \"Your turn\": Show POS and Chunking labels for the same sentences\n",
        "\n",
        "Change `label_field` below to `\"pos_tags\"` or `\"chunk_tags\"` to explore other annotation layers.\n"
      ],
      "metadata": {
        "id": "ZmVGRL18DaKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_word_labels(idx, label_field=\"ner_tags\"):\n",
        "    words = raw_datasets[\"train\"][idx][\"tokens\"]\n",
        "    labels = raw_datasets[\"train\"][idx][label_field]\n",
        "    names = raw_datasets[\"train\"].features[label_field].feature.names\n",
        "    output_word = \"\"\n",
        "    output_label = \"\"\n",
        "    for w, l in zip(words, labels):\n",
        "        name = names[l]\n",
        "        length = max(len(w), len(name))\n",
        "        output_word += w + \" \" * (length - len(w) + 1)\n",
        "        output_label += name + \" \" * (length - len(name) + 1)\n",
        "    print(output_word)\n",
        "    print(output_label)\n",
        "    print()\n",
        "\n",
        "# Your turn: print POS and chunk tags for first and fourth examples\n",
        "print_word_labels(0, \"pos_tags\")\n",
        "print_word_labels(0, \"chunk_tags\")\n",
        "print_word_labels(4, \"pos_tags\")\n",
        "print_word_labels(4, \"chunk_tags\")\n"
      ],
      "metadata": {
        "id": "shXodjMpDYtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Tokenizer: Fast Subword Mapping and Label Alignment\n",
        "\n",
        "Use a BERT tokenizer (fast) and align token-level labels to subwords.\n"
      ],
      "metadata": {
        "id": "S8eCBylWDlZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint=\"bert-base-cased\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "print(\"Is fast tokenizer?\",tokenizer.is_fast)\n",
        "\n",
        "# Tokenize pre-tokenized words\n",
        "inputs=tokenizer(raw_datasets[\"train\"][0][\"tokens\"],is_split_into_words=True)\n",
        "print(\"Tokenized:\",inputs.tokens())\n",
        "print(\"Word IDs:\",inputs.word_ids())  # Word ‚Üí token mapping (None = special token)"
      ],
      "metadata": {
        "id": "JmQ1VdTuDqRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Align word-level labels to tokens (NER example)\n",
        "def align_labels_with_tokens(labels, word_ids):\n",
        "    new_labels = []\n",
        "    current_word = None\n",
        "    for word_id in word_ids:\n",
        "        if word_id != current_word:\n",
        "            current_word = word_id\n",
        "            label = -100 if word_id is None else labels[word_id]\n",
        "            new_labels.append(label)\n",
        "        elif word_id is None:\n",
        "            new_labels.append(-100)\n",
        "        else:\n",
        "            label = labels[word_id]\n",
        "            # If 'B-XXX' class (odd index), switch to I\n",
        "            if label % 2 == 1:\n",
        "                label += 1\n",
        "            new_labels.append(label)\n",
        "    return new_labels\n",
        "\n",
        "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
        "word_ids = inputs.word_ids()\n",
        "aligned_labels = align_labels_with_tokens(labels, word_ids)\n",
        "print(\"Original labels:\", labels)\n",
        "print(\"Aligned token labels:\", aligned_labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "iM3hYKbPFAaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### \"Your turn\": Assign only one label per word, and -100 for all other subtokens.\n"
      ],
      "metadata": {
        "id": "m4gwCYWGHJfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def align_labels_single_per_word(labels, word_ids):\n",
        "    new_labels = []\n",
        "    current_word = None\n",
        "    for word_id in word_ids:\n",
        "        if word_id != current_word:\n",
        "            current_word = word_id\n",
        "            label = -100 if word_id is None else labels[word_id]\n",
        "        else:\n",
        "            label = -100 if word_id is not None else -100\n",
        "        new_labels.append(label)\n",
        "    return new_labels\n",
        "\n",
        "print(\"Single-label per word:\", align_labels_single_per_word(labels, word_ids))\n"
      ],
      "metadata": {
        "id": "BIyfPO63HWs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Preprocessing: Mapping across dataset splits\n",
        "\n",
        "Apply tokenization and alignment to all samples using batched map.\n"
      ],
      "metadata": {
        "id": "BGgfV7r0HgjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "  tokenized_inputs=tokenizer(\n",
        "      examples[\"tokens\"],truncation=True,is_split_into_words=True\n",
        "  )\n",
        "  new_labels=[\n",
        "      align_labels_with_tokens(labels,tokenized_inputs.word_ids(i))\n",
        "      for i,labels in enumerate(examples[\"ner_tags\"])\n",
        "  ]\n",
        "  tokenized_inputs[\"labels\"]=new_labels\n",
        "  return tokenized_inputs\n",
        "\n",
        "tokenized_datasets=raw_datasets.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"train\"].column_names\n",
        ")"
      ],
      "metadata": {
        "id": "AlkB-u62Hn9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Data Collator for Token Classification\n",
        "\n",
        "Pad both inputs and labels (with -100 for padding/subtokens), producing tensors for training.\n"
      ],
      "metadata": {
        "id": "ywg-2575I3n-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "batch=data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
        "print(batch[\"labels\"])"
      ],
      "metadata": {
        "id": "0yQ9wXT4I-SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6Ô∏è‚É£ Metrics: F1, Precision, Recall, Accuracy using SeqEval\n",
        "\n",
        "Evaluate token classification predictions with seqeval.\n"
      ],
      "metadata": {
        "id": "ofHaIl0pJenk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install seqeval"
      ],
      "metadata": {
        "id": "Iy2EktefJkgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n"
      ],
      "metadata": {
        "id": "TWPagxwuNXKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import evaluate\n",
        "metric = evaluate.load(\"seqeval\")\n",
        "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
        "true_str_labels = [label_names[i] for i in labels]\n",
        "\n",
        "fake_pred = true_str_labels.copy()\n",
        "fake_pred[2] = \"O\"\n",
        "print(metric.compute(predictions=[fake_pred], references=[true_str_labels]))\n"
      ],
      "metadata": {
        "id": "6XM33zajN_zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metric Function for Trainer\n",
        "\n",
        "Aggregates metrics from all predictions/labels.\n"
      ],
      "metadata": {
        "id": "20uG7EvAKti1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "  logits,labels=eval_preds\n",
        "  predictions=np.argmax(logits,axis=-1)\n",
        "  true_labels=[[label_names[l] for l in label if l!=-100] for label in labels]\n",
        "  true_predictions=[\n",
        "      [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "      for prediction, label in zip(predictions, labels)\n",
        "  ]\n",
        "\n",
        "  all_metrics=metric.compute(predictions=true_predictions,references=true_labels)\n",
        "  return{\n",
        "      \"precision\": all_metrics[\"overall_precision\"],\n",
        "       \"recall\": all_metrics[\"overall_recall\"],\n",
        "       \"f1\": all_metrics[\"overall_f1\"],\n",
        "       \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
        "  }"
      ],
      "metadata": {
        "id": "-anZhp5oKz0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ccs6iwAO91Jb"
      },
      "outputs": [],
      "source": [
        "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
        "labels = [label_names[i] for i in labels]\n",
        "labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7Ô∏è‚É£ Model Setup\n",
        "\n",
        "Define the BERT token classification model with label mappings.\n"
      ],
      "metadata": {
        "id": "hlD9_XVVMAZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "id2label = {i: label for i, label in enumerate(label_names)}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "print(\"Model label count:\", model.config.num_labels)\n"
      ],
      "metadata": {
        "id": "8pNyrKjCME4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8Ô∏è‚É£ Training Arguments and Trainer\n",
        "\n",
        "Log in, set up Trainer, then fine-tune!\n"
      ],
      "metadata": {
        "id": "D8XEwI14MIAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "args = TrainingArguments(\n",
        "    \"bert-finetuned-ner\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()\n",
        "trainer.push_to_hub(commit_message=\"Training complete\")\n"
      ],
      "metadata": {
        "id": "moiLq4gPMJrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9Ô∏è‚É£ Inference with the Fine-Tuned Model\n",
        "\n",
        "Test your fine-tuned pipeline as shown in the course!\n"
      ],
      "metadata": {
        "id": "RZj9ip17MTzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_checkpoint = \"huggingface-course/bert-finetuned-ner\"\n",
        "token_classifier = pipeline(\n",
        "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
        ")\n",
        "\n",
        "result = token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "4bmhmmlnMVMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JsoRLQb91Jb"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8,\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ1oE9_191Jb"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBZRjSfT91Je"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hL_YfuB_91Jf"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator()\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IekOw6o791Jf"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_train_epochs = 3\n",
        "num_update_steps_per_epoch = len(train_dataloader)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9OhGq7l91Jf"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import Repository, get_full_repo_name\n",
        "\n",
        "model_name = \"bert-finetuned-ner-accelerate\"\n",
        "repo_name = get_full_repo_name(model_name)\n",
        "repo_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyQG26_z91Jf"
      },
      "outputs": [],
      "source": [
        "output_dir = \"bert-finetuned-ner-accelerate\"\n",
        "repo = Repository(output_dir, clone_from=repo_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNcJyroL91Jf"
      },
      "outputs": [],
      "source": [
        "def postprocess(predictions, labels):\n",
        "    predictions = predictions.detach().cpu().clone().numpy()\n",
        "    labels = labels.detach().cpu().clone().numpy()\n",
        "\n",
        "    # Remove ignored index (special tokens) and convert to labels\n",
        "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    return true_labels, true_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDGm6MMk91Jf"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    for batch in eval_dataloader:\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        predictions = outputs.logits.argmax(dim=-1)\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        # Necessary to pad predictions and labels for being gathered\n",
        "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
        "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
        "\n",
        "        predictions_gathered = accelerator.gather(predictions)\n",
        "        labels_gathered = accelerator.gather(labels)\n",
        "\n",
        "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
        "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
        "\n",
        "    results = metric.compute()\n",
        "    print(\n",
        "        f\"epoch {epoch}:\",\n",
        "        {\n",
        "            key: results[f\"overall_{key}\"]\n",
        "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # Save and upload\n",
        "    accelerator.wait_for_everyone()\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
        "    if accelerator.is_main_process:\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        repo.push_to_hub(\n",
        "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_BzaAfs91Jf"
      },
      "outputs": [],
      "source": [
        "accelerator.wait_for_everyone()\n",
        "unwrapped_model = accelerator.unwrap_model(model)\n",
        "unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-p_b_p-91Jf"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Replace this with your own checkpoint\n",
        "model_checkpoint = \"huggingface-course/bert-finetuned-ner\"\n",
        "token_classifier = pipeline(\n",
        "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
        ")\n",
        "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Token classification (PyTorch)",
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}