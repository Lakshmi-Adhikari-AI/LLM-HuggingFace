{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wLEpyfgRqMX"
      },
      "source": [
        "# üóÉÔ∏è Big data? ü§ó Datasets to the rescue!\n",
        "\n",
        "Modern NLP workloads often face \"too big for RAM\" datasets.  \n",
        "Hugging Face Datasets handles multi-gigabyte and even terabyte-scale corpora with memory mapping and streaming.\n",
        "\n",
        "We'll use the 825GB \"Pile\" corpus, starting with PubMed abstracts‚Äîa 19GB biomedical data subset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBTR6eljRqMZ"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKfWqMT1RqMa"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ Install For Big Data: zstandard Compression\n",
        "\n",
        "First, install the zstandard library used for decompressing the dataset files.\n"
      ],
      "metadata": {
        "id": "hV3cvTvbSJil"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8WJ5JQERqMb"
      },
      "outputs": [],
      "source": [
        "!pip install zstandard"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Load a Massive Dataset\n",
        "\n",
        "Let's load the PubMed abstracts subset (15 million medical texts, ~20GB).\n",
        "Even with this size, ü§ó Datasets lets you work efficiently and safely.\n"
      ],
      "metadata": {
        "id": "BbTAz8doSS3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# URL to the compressed PubMed subset\n",
        "data_files = \"https://huggingface.co/datasets/qualis2006/PUBMED_title_abstracts_2020_baseline/resolve/main/PUBMED_title_abstracts_2020_baseline.jsonl.zst\"\n",
        "pubmed_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\n",
        "pubmed_dataset\n"
      ],
      "metadata": {
        "id": "lblG1JXuSUqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Inspect the First Example\n",
        "\n",
        "Let's check the format and content of a single PubMed entry.\n"
      ],
      "metadata": {
        "id": "sB_B2-EFTJUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Peek at the very first row in the dataset\n",
        "print(pubmed_dataset[0])"
      ],
      "metadata": {
        "id": "_7wXdl97TMdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Measure RAM Usage with psutil\n",
        "\n",
        "Despite the dataset's huge size, RAM usage stays moderate‚Äîthanks to memory mapping.\n"
      ],
      "metadata": {
        "id": "dHClH1HVTdwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install psutil"
      ],
      "metadata": {
        "id": "ISQGzqw8bBlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "\n",
        "# Process.memory_info is expressed in bytes, so convert to megabytes\n",
        "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n"
      ],
      "metadata": {
        "id": "1f_w9gE4TsYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Dataset On-Disk Size vs. RAM\n",
        "\n",
        "How big is the Arrow cache file? Compare RAM and disk footprint below.\n"
      ],
      "metadata": {
        "id": "XJwX007hUTTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of files in dataset : {pubmed_dataset.dataset_size}\")\n",
        "size_gb = pubmed_dataset.dataset_size / (1024**3)\n",
        "print(f\"Dataset size (cache file) : {size_gb:.2f} GB\")"
      ],
      "metadata": {
        "id": "2R_kb5fTUXO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6Ô∏è‚É£ Speed Test: Iterate Over the Whole Dataset\n",
        "\n",
        "Let's measure sequential read speed from disk into RAM using memory mapping and Arrow.\n"
      ],
      "metadata": {
        "id": "iQOkZdRfU1eZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "\n",
        "code_snippet = \"\"\"batch_size = 1000\n",
        "\n",
        "for idx in range(0, len(pubmed_dataset), batch_size):\n",
        "    _ = pubmed_dataset[idx:idx + batch_size]\n",
        "\"\"\"\n",
        "\n",
        "time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())\n",
        "print(\n",
        "    f\"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in \"\n",
        "    f\"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "I0WX7jv-VAJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7Ô∏è‚É£ Streaming Datasets\n",
        "\n",
        "For *really* big data (larger than local disk!), load in streaming mode to fetch samples on-demand.\n",
        "\n",
        "This keeps only a small subset in memory and never stores the full dataset locally.\n"
      ],
      "metadata": {
        "id": "dKWrBSzmVwv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pubmed_dataset_streamed=load_dataset(\n",
        "    \"json\",data_files=data_files,split=\"train\",streaming=True\n",
        ")\n",
        "\n",
        "# pubmed_dataset_streamed is an IterableDataset,so use iter and next\n",
        "print(next(iter(pubmed_dataset_streamed)))"
      ],
      "metadata": {
        "id": "rfu0_t1qV42l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8Ô∏è‚É£ On-the-fly Tokenization With Streaming\n",
        "\n",
        "Streamed datasets can be mapped over and tokenized in batches, just like regular datasets.\n",
        "\n",
        "We'll use a pretrained tokenizer.\n"
      ],
      "metadata": {
        "id": "h1YyVGVUWW-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer=AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Tokenize each streamed example as its read\n",
        "tokenized_dataset=pubmed_dataset_streamed.map(lambda x:tokenizer(x[\"text\"]))\n",
        "print(next(iter(tokenized_dataset)))"
      ],
      "metadata": {
        "id": "F0qUOVVAWbrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9Ô∏è‚É£ Buffer-based Shuffling with Streaming\n",
        "\n",
        "Shuffling is available for streamed data, but only within a buffer (not global random-shuffle).\n"
      ],
      "metadata": {
        "id": "pdPAabfCXVZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_dataset=pubmed_dataset_streamed.shuffle(buffer_size=10_000,seed=42)\n",
        "print(next(iter(shuffled_dataset)))"
      ],
      "metadata": {
        "id": "EILpME3_XaGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîü \"Take\" and \"Skip\" for Streams\n",
        "\n",
        "Use `.take(N)` to preview or split data from a streamed source, and `.skip(N)` to discard/partition efficiently.\n"
      ],
      "metadata": {
        "id": "qy2b-rGDXvK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the first 5 examples\n",
        "dataset_head=pubmed_dataset_streamed.take(5)\n",
        "list(dataset_head)"
      ],
      "metadata": {
        "id": "g_eTcVPbXysB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Skip the first 1,000 examples and include the rest in the training set\n",
        "train_dataset = shuffled_dataset.skip(1000)\n",
        "# Take the first 1,000 examples for the validation set\n",
        "validation_dataset = shuffled_dataset.take(1000)"
      ],
      "metadata": {
        "id": "nPXZxDHrfOYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£1Ô∏è‚É£ Interleaving Huge Datasets On the Fly\n",
        "\n",
        "Want to mix multiple massive sources? Use `interleave_datasets` for fair round-robin reading.\n"
      ],
      "metadata": {
        "id": "lOCT7CBlYDKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import interleave_datasets\n",
        "from itertools import islice\n",
        "\n",
        "# Load another large streamed subset, e.g FreeLaw (51GB legal cases)\n",
        "law_dataset_streamed = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst\",\n",
        "    split=\"train\",\n",
        "    streaming=True,\n",
        ")\n",
        "next(iter(law_dataset_streamed))\n",
        "\n",
        "# Interleave PubMed and FreeLaw streams\n",
        "combined_dataset=interleave_datasets([pubmed_dataset_streamed,law_dataset_streamed])\n",
        "print(list(islice(combined_dataset,2)))"
      ],
      "metadata": {
        "id": "KEwbyZcaYLe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£2Ô∏è‚É£ Streaming the Entire Pile (825GB)\n",
        "\n",
        "With URLs and streaming, even the largest datasets are feasible (if high network throughput is available!).\n"
      ],
      "metadata": {
        "id": "mcjwqJpVZkPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_url=\"https://the-eye.eu/public/AI/pile/\"\n",
        "data_files={\n",
        "    \"train\":[base_url+\"train/\"+f\"{idx:02d}.jsonl.zst\" for idx in range(30)],\n",
        "    \"validation\":base_url+\"val.jsonl.zst\",\n",
        "    \"test\":base_url+\"test.jsonl.zst\",\n",
        "}\n",
        "pile_dataset=load_dataset(\"json\",data_files=data_files,streaming=True)\n",
        "print(next(iter(pile_dataset(\"train\"))))"
      ],
      "metadata": {
        "id": "4MGVt3j9ZpYc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Big data? ü§ó Datasets to the rescue!",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}