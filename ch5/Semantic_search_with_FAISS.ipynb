{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujcEFgWnJAJI"
      },
      "source": [
        "# üîç Semantic Search over GitHub Issues Using Embeddings and FAISS\n",
        "\n",
        "In this notebook, we build a powerful, modern search engine for GitHub issues using text embeddings and FAISS‚Äîall with Hugging Face Datasets, Transformers, and PyTorch.\n",
        "Our goal: Given a natural language question, retrieve the most helpful comments from the issues corpus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3gv68x7JAJS"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wUpPUEkJAJV"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ Load and Filter the GitHub Issues Dataset\n",
        "\n",
        "We‚Äôll use the dataset of issues/comments pushed to the Hub in the previous section.\n",
        "We'll filter out pull requests and issues without comments, since those aren't useful as answers.\n"
      ],
      "metadata": {
        "id": "JRSpsVmAPG9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the custom GitHub issues dataset(change user as appropriate)\n",
        "issues_dataset = load_dataset(\"lewtun/github-issues\",split=\"train\")\n",
        "print(issues_dataset)\n",
        "\n",
        "# Remove pull requests and issues with no comments\n",
        "issues_dataset = issues_dataset.filter(\n",
        "    lambda x: (x[\"is_pull_request\"]==False and len(x[\"comments\"])>0)\n",
        ")\n",
        "print(issues_dataset)"
      ],
      "metadata": {
        "id": "e8Wicy3nPRGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Keep Only Relevant Columns\n",
        "\n",
        "For search, keep only `title`, `body`, `comments`, and `html_url` (for referencing the original issue).\n"
      ],
      "metadata": {
        "id": "oNZEVaVBQnoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns=issues_dataset.column_names\n",
        "columns_to_keep=[\"title\",\"body\",\"html_url\",\"comments\"]\n",
        "columns_to_remove=set(columns)-set(columns_to_keep)\n",
        "issues_dataset=issues_dataset.remove_columns(list(columns_to_remove))\n",
        "print(issues_dataset)\n",
        "print(issues_dataset[0]['comments'])"
      ],
      "metadata": {
        "id": "b3FRJlfwQsVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Explode the Comments (One Row Per Comment)\n",
        "\n",
        "Each issue may have multiple comments. To support matching queries to comments, explode this list so every row is a single comment/context pair.\n"
      ],
      "metadata": {
        "id": "7_Z06_WgRt95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list of comments in one row\n",
        "print(issues_dataset[0]['comments'])"
      ],
      "metadata": {
        "id": "pG8eDL_xSho8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to pandas,explode comments\n",
        "issues_dataset.set_format(\"pandas\")\n",
        "df=issues_dataset[:]\n",
        "comments_df=df.explode(\"comments\",ignore_index=True)\n",
        "comments_df.head(4)"
      ],
      "metadata": {
        "id": "jHDg15lARy8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Return to Datasets and Clean by Comment Length\n",
        "\n",
        "Convert back to a Dataset and keep only comments longer than 15 words (to remove unhelpful, super-short messages).\n"
      ],
      "metadata": {
        "id": "hhN78fGJTZKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "comments_dataset=Dataset.from_pandas(comments_df)\n",
        "comments_dataset=comments_dataset.map(\n",
        "    lambda x: {\"comments_length\":len(x[\"comments\"].split())}\n",
        ")\n",
        "comments_dataset = comments_dataset.filter(lambda x:x[\"comments_length\"]>15)\n",
        "print(comments_dataset)"
      ],
      "metadata": {
        "id": "zKn7g38HThP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Concatenate All Context into a Single Text Field\n",
        "\n",
        "Join together the issue title, body, and comment for full queryable context.\n"
      ],
      "metadata": {
        "id": "2GzKy-FgUZNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concatenate_text(examples):\n",
        "  return {\n",
        "      \"text\":examples[\"title\"]+\"\\n\"+examples[\"body\"]+\"\\n\"+examples[\"comments\"]\n",
        "  }\n",
        "  comments_dataset=comments_dataset.map(concatenate_text)\n",
        ""
      ],
      "metadata": {
        "id": "JWzp74SOUp5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6Ô∏è‚É£ Compute Text Embeddings with Sentence Transformers\n",
        "\n",
        "We can now produce a single embedding per context using a pretrained model.\n",
        "We'll use `sentence-transformers/multi-qa-mpnet-base-dot-v1` recommended for QA search.\n"
      ],
      "metadata": {
        "id": "3y6jFWAFV34r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer,AutoModel\n",
        "import torch\n",
        "\n",
        "model_ckpt=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model=AutoModel.from_pretrained(model_ckpt)\n",
        "\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# CLS pooling function: take the embedding of the special [CLS] token\n",
        "def cls_pooling(model_output):\n",
        "  return model_output.last_hidden_state[:,0]\n",
        "\n",
        "def get_embeddings(text_list):\n",
        "  encoded_input=tokenizer(text_list,padding=True,truncation=True,return_tensors=\"pt\")\n",
        "  encoded_input={k:v.to(device) for k,v in encoded_input.itmes()}\n",
        "  with torch.no_grad():\n",
        "    model_output = model(**encoded_input)\n",
        "  return cls_pooling(model_output)"
      ],
      "metadata": {
        "id": "QRRnnlvaWGhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7Ô∏è‚É£ Compute and Attach Embeddings to Each Row\n",
        "\n",
        "Use `.map()` to embed all the documents and store as numpy arrays.\n"
      ],
      "metadata": {
        "id": "EcA0jbwkYJc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_dataset = comments_dataset.map(\n",
        "    lambda x: {\"embeddings\": get_embeddings([x[\"text\"]]).cpu().numpy()[0]}\n",
        ")\n"
      ],
      "metadata": {
        "id": "X9j0soKAYZ1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8Ô∏è‚É£ Build the FAISS Index for Efficient Search\n",
        "\n",
        "Now, add a FAISS index for the embeddings (automatically uses the \"embeddings\" column).\n"
      ],
      "metadata": {
        "id": "RDALJn3WYwUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_dataset.add_faiss_index(column=\"embeddings\")\n"
      ],
      "metadata": {
        "id": "7Srf_3qiY6R4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9Ô∏è‚É£ Test Your Search Engine with a Query\n",
        "\n",
        "Embed a new question and retrieve the top k most relevant comments using nearest-neighbors search.\n",
        "\n"
      ],
      "metadata": {
        "id": "Cpfei_ODZBQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How can I load a dataset offline?\"\n",
        "question_embedding = get_embeddings([question]).cpu().numpy()\n",
        "\n",
        "# Query the FAISS index for the 5 nearest neighbors\n",
        "scores, samples = embeddings_dataset.get_nearest_examples(\n",
        "    \"embeddings\", question_embedding, k=5\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "OBG0jOOqZheb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîü Review the Results\n",
        "\n",
        "Visualize which comments matched the query, including the score and the issue context/title.\n"
      ],
      "metadata": {
        "id": "fv9WNTX2ct0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "samples_df=pd.DataFrame.from_dict(samples)\n",
        "samples_df[\"scores\"]=scores\n",
        "samples_df.sort_values(\"scores\",ascending=False,inplace=True)\n",
        "\n",
        "for _,row in samples_df.iterrows():\n",
        "  print(f\"COMMENT:\\n{row.comments}\\n\")\n",
        "  print(f\"SCORE: {row.scores}\")\n",
        "  print(f\"TITLE: {row.title}\")\n",
        "  print(f\"URL: {row.html_url}\")\n",
        "  print(\"=\" * 50)"
      ],
      "metadata": {
        "id": "eNrMNRrNcw0k"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Semantic search with FAISS (PyTorch)",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}