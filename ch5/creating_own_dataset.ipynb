{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AbAVt_GGrGB"
      },
      "source": [
        "# üì¶ Creating Your Own Dataset with ü§ó Datasets\n",
        "\n",
        "Not every dataset you need exists on the Hugging Face Hub!  \n",
        "Let's go step by step through creating a custom NLP dataset:  \n",
        "We'll fetch GitHub issues from the ü§ó Datasets repository, clean & augment them, and push the final result to the Hub for the world to use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fOP_HEjGrGD"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUQyNLwZGrGD"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tIAgjmqGrGE"
      },
      "source": [
        "You will need to setup git, adapt your email and name in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeJqGk9CGrGE"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"lakshmi.adhikari26@gmail.com\"\n",
        "!git config --global user.name \"Lakshmi-Adhikari-AI\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLsiXOZ7GrGF"
      },
      "source": [
        "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqqwetKUGrGF"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the data\n",
        "How to fetch GitHub issues data using the GitHub API."
      ],
      "metadata": {
        "id": "mqTJqb5-I5eV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42CcXEfCGrGG"
      },
      "outputs": [],
      "source": [
        "# Install requests library(for making HTTP calls)\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obd25Hz2GrGH"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=1\"\n",
        "response = requests.get(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRL5SVwLGrGH"
      },
      "outputs": [],
      "source": [
        "response.status_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgZxlXx9GrGI"
      },
      "outputs": [],
      "source": [
        "response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ Install and Import Requirements\n",
        "\n",
        "We'll use `requests` to fetch data from the GitHub API, and `tqdm`, `math`, `pandas`, and `datasets` for data processing.\n"
      ],
      "metadata": {
        "id": "M7Kp9vq7J5CP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install requests if not present\n",
        "!pip install requests tqdm\n",
        "\n",
        "import requests\n",
        "import time\n",
        "import math\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "from datasets import load_dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "qLVQgyBoKEXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Getting Data from the GitHub API\n",
        "\n",
        "We retrieve issues (and pull requests) from the ü§ó Datasets repo using the GitHub REST API.  \n",
        "We recommend authenticating with a GitHub [Personal Access Token](https://github.com/settings/tokens)‚Äîthis increases the rate limit from 60 to 5000 requests/hour.\n"
      ],
      "metadata": {
        "id": "5RujHWlHKUhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill in your GitHub token\n",
        "GITHUB_TOKEN=\"your GitHub token\"\n",
        "headers={\"Authorization\":f\"token{GITHUB_TOKEN}\"}"
      ],
      "metadata": {
        "id": "X3POePG5Lpl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Define a Function to Download All Issues (and Pull Requests)\n",
        "\n",
        "This function will paginate through issues, avoid rate limits, and save the raw data to a JSONL file for later use.\n"
      ],
      "metadata": {
        "id": "xulvdfOrMEyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def fetch_issues(\n",
        "    owner=\"huggingface\",\n",
        "    repo=\"datasets\",\n",
        "    num_issues=10_000,\n",
        "    rate_limit=5_000,\n",
        "    issues_path=Path(\".\"),\n",
        "):\n",
        "    if not issues_path.is_dir():\n",
        "        issues_path.mkdir(exist_ok=True)\n",
        "\n",
        "    batch = []\n",
        "    all_issues = []\n",
        "    per_page = 100  # Number of issues to return per page\n",
        "    num_pages = math.ceil(num_issues / per_page)\n",
        "    base_url = \"https://api.github.com/repos\"\n",
        "\n",
        "    for page in tqdm(range(num_pages)):\n",
        "        # Query with state=all to get both open and closed issues\n",
        "        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n",
        "        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n",
        "        batch.extend(issues.json())\n",
        "\n",
        "        if len(batch) > rate_limit and len(all_issues) < num_issues:\n",
        "            all_issues.extend(batch)\n",
        "            batch = []  # Flush batch for next time period\n",
        "            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n",
        "            time.sleep(60 * 60 + 1)\n",
        "\n",
        "    all_issues.extend(batch)\n",
        "    df = pd.DataFrame.from_records(all_issues)\n",
        "    df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True)\n",
        "    print(\n",
        "        f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.json\"\n",
        "    )"
      ],
      "metadata": {
        "id": "e8m_DFcjMPah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Downloading All Issues from ü§ó Datasets\n",
        "\n",
        "‚ö†Ô∏è Depending on your connection, this may take a few minutes.  \n",
        "Afterwards, you'll have a `datasets-issues.jsonl` file.\n"
      ],
      "metadata": {
        "id": "8OB05Y2NPavT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sySPDnZVGrGI"
      },
      "outputs": [],
      "source": [
        "fetch_issues()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Load the Issues Locally as a Hugging Face Dataset\n",
        "\n",
        "We'll use `load_dataset` to read the JSON lines file as a Dataset object for further analysis.\n"
      ],
      "metadata": {
        "id": "TF9rYpEvQA4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "issues_dataset = load_dataset(\"json\", data_files=\"datasets-issues.jsonl\", split=\"train\")\n",
        "print(issues_dataset)\n"
      ],
      "metadata": {
        "id": "L_owRJQ8QIgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6Ô∏è‚É£ Distinguishing Issues from Pull Requests\n",
        "\n",
        "GitHub returns both as issues; pull requests have a `pull_request` field.   \n",
        "Let's add an `is_pull_request` boolean column.\n"
      ],
      "metadata": {
        "id": "H-04_mBQQwQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "issues_dataset = issues_dataset.map(\n",
        "    lambda x: {\"is_pull_request\": False if x[\"pull_request\"] is None else True}\n",
        ")\n"
      ],
      "metadata": {
        "id": "u3hHiPe-QznV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7Ô∏è‚É£ Sampling and Inspecting the Structure\n",
        "\n",
        "Shuffle and print three samples for manual inspection.\n"
      ],
      "metadata": {
        "id": "PvWqE8WzRkVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample=issues_dataset.shuffle(seed=666).select(range(3))\n",
        "for url,pr in zip(sample[\"html_url\"],sample[\"[pull_request]\"]):\n",
        "  print(f\">>URL: {url}\")\n",
        "  print(f\">> Pull request:{pr}\\n\")"
      ],
      "metadata": {
        "id": "agVg4upzRnYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8Ô∏è‚É£ (Optional) Add Comments to Each Issue\n",
        "\n",
        "Let's fetch all comments for each issue, and add them as a \"comments\" column.  \n",
        "**This is network-intensive and can take a while!**\n"
      ],
      "metadata": {
        "id": "KIvPXGO7UNWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_comments(issue_number):\n",
        "  url=f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
        "  response=requests.get(url,headers=headers)\n",
        "  return [r[\"body\"] for r in response.json()]\n",
        "# Add comments to all issues\n",
        "issues_with_comments_dataset=issues_dataset.map(\n",
        "    lambda x: {\"comments\":get_comments(x[\"number\"])}\n",
        ")"
      ],
      "metadata": {
        "id": "UAfLH0oQURdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9Ô∏è‚É£ Push Your Dataset to the Hugging Face Hub\n",
        "\n",
        "Log in to Hugging Face in your notebook, then upload!\n"
      ],
      "metadata": {
        "id": "4U0kQRAfVOQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "# Push to the Hub(replace your_name/dataset_name as needed)\n",
        "issues_with_comments_dataset.push_to_hub(\"Lakshmi26/github-issues\")"
      ],
      "metadata": {
        "id": "GrOxNi6RVRuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfr__Wa4GrGJ"
      },
      "outputs": [],
      "source": [
        "def get_comments(issue_number):\n",
        "    url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
        "    response = requests.get(url, headers=headers)\n",
        "    return [r[\"body\"] for r in response.json()]\n",
        "\n",
        "\n",
        "# Test our function works as expected\n",
        "get_comments(2792)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîü (Optional) Load Your Uploaded Dataset\n",
        "\n",
        "Anyone can now use your dataset as below:\n"
      ],
      "metadata": {
        "id": "dbn6MXdkWQb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remote_dataset=load_dataset(\"Username/github-issues\")\n",
        "print(remote_dataset)"
      ],
      "metadata": {
        "id": "GslH3wXRWSSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîñ Remember: Create a Dataset Card!\n",
        "\n",
        "After pushing, add a `README.md` (\"dataset card\") on the Hub.  \n",
        "Describe the dataset, how it was built, license, intended use, and fields.  \n",
        "Follow the [Hugging Face guide](https://github.com/huggingface/datasets/blob/master/templates/README_guide.md) for best practice templates.\n"
      ],
      "metadata": {
        "id": "W0Nl7U8gWiWP"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Creating your own dataset",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}